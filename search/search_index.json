{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Human Learn \u00b6 Machine Learning models should play by the rules, literally. Project Goal \u00b6 Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground but it is also capable of making bad decision. We've also reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This package contains scikit-learn compatible tools that should make it easier to construct and benchmark rule based systems that are designed by humans. You can also use it in combination with ML models. Install \u00b6 You can install this tool via pip . python - m pip install human - learn Guides \u00b6 Tutorial \u00b6 There is a full course on this tool available on calmcode.io . This is the first video. Getting Started \u00b6 To help you get started we've written some helpful getting started guides. Functions as a Model Human Preprocessing Drawing as a Model Outliers and Comfort Drawing Features You can also check out the API documentation here . Features \u00b6 This library hosts a couple of models that you can play with. Interactive Drawings \u00b6 This tool allows you to draw over your datasets. These drawings can later be converted to models or to preprocessing tools. Classification Models \u00b6 FunctionClassifier \u00b6 This allows you to define a function that can make classification predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. InteractiveClassifier \u00b6 This allows you to draw decision boundaries in interactive charts to create a model. You can create charts interactively in the notebook and export it as a scikit-learn compatible model. Regression Models \u00b6 FunctionRegressor \u00b6 This allows you to define a function that can make regression predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. Outlier Detection Models \u00b6 FunctionOutlierDetector \u00b6 This allows you to define a function that can declare outliers. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. InteractiveOutlierDetector \u00b6 This allows you to draw decision boundaries in interactive charts to create a model. If a point falls outside of these boundaries we might be able to declare it an outlier. There's a threshold parameter for how strict you might want to be. Preprocessing Models \u00b6 PipeTransformer \u00b6 This allows you to define a function that can make handle preprocessing. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. This is especially powerful in combination with the pandas .pipe method. If you're unfamiliar with this amazing feature, you may appreciate this tutorial . InteractivePreprocessor \u00b6 This allows you to draw features that you'd like to add to your dataset or your machine learning pipeline. You can use it via tfm.fit(df).transform(df) and df.pipe(tfm) . Datasets \u00b6 Titanic \u00b6 This library hosts the popular titanic survivor dataset for demo purposes. The goal of this dataset is to predict who might have survived the titanic disaster. Fish \u00b6 The fish market dataset is also hosted in this library. The goal of this dataset is to predict the weight of fish. However, it can also be turned into a classification problem by predicting the species.","title":"Index"},{"location":"index.html#human-learn","text":"Machine Learning models should play by the rules, literally.","title":"Human Learn"},{"location":"index.html#project-goal","text":"Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground but it is also capable of making bad decision. We've also reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This package contains scikit-learn compatible tools that should make it easier to construct and benchmark rule based systems that are designed by humans. You can also use it in combination with ML models.","title":"Project Goal"},{"location":"index.html#install","text":"You can install this tool via pip . python - m pip install human - learn","title":"Install"},{"location":"index.html#guides","text":"","title":"Guides"},{"location":"index.html#tutorial","text":"There is a full course on this tool available on calmcode.io . This is the first video.","title":"Tutorial"},{"location":"index.html#getting-started","text":"To help you get started we've written some helpful getting started guides. Functions as a Model Human Preprocessing Drawing as a Model Outliers and Comfort Drawing Features You can also check out the API documentation here .","title":"Getting Started"},{"location":"index.html#features","text":"This library hosts a couple of models that you can play with.","title":"Features"},{"location":"index.html#interactive-drawings","text":"This tool allows you to draw over your datasets. These drawings can later be converted to models or to preprocessing tools.","title":"Interactive Drawings"},{"location":"index.html#classification-models","text":"","title":"Classification Models"},{"location":"index.html#functionclassifier","text":"This allows you to define a function that can make classification predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.","title":"FunctionClassifier"},{"location":"index.html#interactiveclassifier","text":"This allows you to draw decision boundaries in interactive charts to create a model. You can create charts interactively in the notebook and export it as a scikit-learn compatible model.","title":"InteractiveClassifier"},{"location":"index.html#regression-models","text":"","title":"Regression Models"},{"location":"index.html#functionregressor","text":"This allows you to define a function that can make regression predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.","title":"FunctionRegressor"},{"location":"index.html#outlier-detection-models","text":"","title":"Outlier Detection Models"},{"location":"index.html#functionoutlierdetector","text":"This allows you to define a function that can declare outliers. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.","title":"FunctionOutlierDetector"},{"location":"index.html#interactiveoutlierdetector","text":"This allows you to draw decision boundaries in interactive charts to create a model. If a point falls outside of these boundaries we might be able to declare it an outlier. There's a threshold parameter for how strict you might want to be.","title":"InteractiveOutlierDetector"},{"location":"index.html#preprocessing-models","text":"","title":"Preprocessing Models"},{"location":"index.html#pipetransformer","text":"This allows you to define a function that can make handle preprocessing. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. This is especially powerful in combination with the pandas .pipe method. If you're unfamiliar with this amazing feature, you may appreciate this tutorial .","title":"PipeTransformer"},{"location":"index.html#interactivepreprocessor","text":"This allows you to draw features that you'd like to add to your dataset or your machine learning pipeline. You can use it via tfm.fit(df).transform(df) and df.pipe(tfm) .","title":"InteractivePreprocessor"},{"location":"index.html#datasets","text":"","title":"Datasets"},{"location":"index.html#titanic","text":"This library hosts the popular titanic survivor dataset for demo purposes. The goal of this dataset is to predict who might have survived the titanic disaster.","title":"Titanic"},{"location":"index.html#fish","text":"The fish market dataset is also hosted in this library. The goal of this dataset is to predict the weight of fish. However, it can also be turned into a classification problem by predicting the species.","title":"Fish"},{"location":"api/classification.html","text":"from hulearn.classification import * \u00b6 FunctionClassifier \u00b6 This class allows you to pass a function to make the predictions you're interested in. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! Usage: import numpy as np import pandas as pd from sklearn.model_selection import GridSearchCV from hulearn.datasets import load_titanic from hulearn.classification import FunctionClassifier df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] def class_based ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) mod = FunctionClassifier ( class_based , pclass = 10 ) params = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]} grid = GridSearchCV ( mod , cv = 3 , param_grid = params ) . fit ( X , y ) pd . DataFrame ( grid . cv_results_ ) fit ( self , X , y ) \u00b6 Show source code in classification/functionclassifier.py 43 44 45 46 47 48 49 50 def fit ( self , X , y ): \"\"\" Fit the classifier. No-Op. \"\"\" # Run it to confirm no error happened. self . fitted_ = True _ = self . func ( X , ** self . kwargs ) return self Fit the classifier. No-Op. partial_fit ( self , X , y , classes = None , sample_weight = None ) \u00b6 Show source code in classification/functionclassifier.py 52 53 54 55 56 57 58 59 def partial_fit ( self , X , y , classes = None , sample_weight = None ): \"\"\" Fit the classifier partially. No-Op. \"\"\" # Run it to confirm no error happened. self . fitted_ = True _ = self . func ( X , ** self . kwargs ) return self Fit the classifier partially. No-Op. predict ( self , X ) \u00b6 Show source code in classification/functionclassifier.py 61 62 63 64 65 66 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function. InteractiveClassifier \u00b6 This tool allows you to take a drawn model and use it as a classifier. Parameters Name Type Description Default json_desc chart data in dictionary form required smoothing smoothing to apply to poly-counts 0.001 refit if True , you no longer need to call .fit(X, y) in order to .predict(X) True Usage: from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) charts = InteractiveCharts ( df , labels = \"species\" ) # Next notebook cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Next notebook cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) # After drawing a model, export the data json_data = charts . data () # You can now use your drawn intuition as a model! from hulearn.classification.interactive import InteractiveClassifier clf = InteractiveClassifier ( clf_data ) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. # It can also be used in `GridSearchCV` for benchmarking! clf . predict ( X ) fit ( self , X , y ) \u00b6 Show source code in classification/interactiveclassifier.py 114 115 116 117 118 119 120 def fit ( self , X , y ): \"\"\" Fit the classifier. Bit of a formality, it's not doing anything specifically. \"\"\" self . classes_ = list ( self . json_desc [ 0 ][ \"polygons\" ] . keys ()) self . fitted_ = True return self Fit the classifier. Bit of a formality, it's not doing anything specifically. from_json ( path , smoothing = 0.001 , refit = True ) (classmethod) \u00b6 Show source code in classification/interactiveclassifier.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @classmethod def from_json ( cls , path , smoothing = 0.001 , refit = True ): \"\"\" Load the classifier from json stored on disk. Arguments: path: path of the json file smoothing: smoothing to apply to poly-counts refit: if `True`, you no longer need to call `.fit(X, y)` in order to `.predict(X)` Usage: ```python from hulearn.classification import InteractiveClassifier InteractiveClassifier.from_json(\"path/to/file.json\") ``` \"\"\" json_desc = json . loads ( pathlib . Path ( path ) . read_text ()) return InteractiveClassifier ( json_desc = json_desc , smoothing = smoothing , refit = refit ) Load the classifier from json stored on disk. Parameters Name Type Description Default path path of the json file required smoothing smoothing to apply to poly-counts 0.001 refit if True , you no longer need to call .fit(X, y) in order to .predict(X) True Usage: from hulearn.classification import InteractiveClassifier InteractiveClassifier . from_json ( \"path/to/file.json\" ) predict ( self , X ) \u00b6 Show source code in classification/interactiveclassifier.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def predict ( self , X ): \"\"\" Predicts the class for each item in `X`. Usage: ```python from hulearn.classification import InteractiveClassifier clf = InteractiveClassifier(clf_data) X, y = load_data(...) # This doesn't do anything. But scikit-learn demands it. clf.fit(X, y) # This makes predictions, based on your drawn model. clf.predict(X) ``` \"\"\" check_is_fitted ( self , [ \"classes_\" , \"fitted_\" ]) return np . array ( [ self . classes_ [ i ] for i in self . predict_proba ( X ) . argmax ( axis = 1 )] ) Predicts the class for each item in X . Usage: from hulearn.classification import InteractiveClassifier clf = InteractiveClassifier ( clf_data ) X , y = load_data ( ... ) # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. clf . predict ( X ) predict_proba ( self , X ) \u00b6 Show source code in classification/interactiveclassifier.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def predict_proba ( self , X ): \"\"\" Predicts the associated probabilities for each class. Usage: ```python from hulearn.classification import InteractiveClassifier clf = InteractiveClassifier(clf_data) X, y = load_data(...) # This doesn't do anything. But scikit-learn demands it. clf.fit(X, y) # This makes predictions, based on your drawn model. clf.predict_proba(X) ``` \"\"\" # Because we're not doing anything during training, for convenience this # method can formally \"fit\" during the predict call. This is a scikit-learn # anti-pattern so we allow you to turn this off. if self . refit : if not self . fitted_ : self . fit ( X ) check_is_fitted ( self , [ \"classes_\" , \"fitted_\" ]) if isinstance ( X , pd . DataFrame ): hits = [ self . _count_hits ( self . poly_data , x [ 1 ] . to_dict ()) for x in X . iterrows () ] else : hits = [ self . _count_hits ( self . poly_data , { k : v for k , v in enumerate ( x )}) for x in X ] count_arr = ( np . array ([[ h [ c ] for c in self . classes_ ] for h in hits ]) + self . smoothing ) return count_arr / count_arr . sum ( axis = 1 ) . reshape ( - 1 , 1 ) Predicts the associated probabilities for each class. Usage: from hulearn.classification import InteractiveClassifier clf = InteractiveClassifier ( clf_data ) X , y = load_data ( ... ) # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. clf . predict_proba ( X )","title":"Classification"},{"location":"api/classification.html#from-hulearnclassification-import","text":"","title":"from hulearn.classification import *"},{"location":"api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier","text":"This class allows you to pass a function to make the predictions you're interested in. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! Usage: import numpy as np import pandas as pd from sklearn.model_selection import GridSearchCV from hulearn.datasets import load_titanic from hulearn.classification import FunctionClassifier df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] def class_based ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) mod = FunctionClassifier ( class_based , pclass = 10 ) params = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]} grid = GridSearchCV ( mod , cv = 3 , param_grid = params ) . fit ( X , y ) pd . DataFrame ( grid . cv_results_ )","title":"FunctionClassifier"},{"location":"api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.fit","text":"Show source code in classification/functionclassifier.py 43 44 45 46 47 48 49 50 def fit ( self , X , y ): \"\"\" Fit the classifier. No-Op. \"\"\" # Run it to confirm no error happened. self . fitted_ = True _ = self . func ( X , ** self . kwargs ) return self Fit the classifier. No-Op.","title":"fit()"},{"location":"api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.partial_fit","text":"Show source code in classification/functionclassifier.py 52 53 54 55 56 57 58 59 def partial_fit ( self , X , y , classes = None , sample_weight = None ): \"\"\" Fit the classifier partially. No-Op. \"\"\" # Run it to confirm no error happened. self . fitted_ = True _ = self . func ( X , ** self . kwargs ) return self Fit the classifier partially. No-Op.","title":"partial_fit()"},{"location":"api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.predict","text":"Show source code in classification/functionclassifier.py 61 62 63 64 65 66 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"predict()"},{"location":"api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier","text":"This tool allows you to take a drawn model and use it as a classifier. Parameters Name Type Description Default json_desc chart data in dictionary form required smoothing smoothing to apply to poly-counts 0.001 refit if True , you no longer need to call .fit(X, y) in order to .predict(X) True Usage: from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) charts = InteractiveCharts ( df , labels = \"species\" ) # Next notebook cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Next notebook cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) # After drawing a model, export the data json_data = charts . data () # You can now use your drawn intuition as a model! from hulearn.classification.interactive import InteractiveClassifier clf = InteractiveClassifier ( clf_data ) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. # It can also be used in `GridSearchCV` for benchmarking! clf . predict ( X )","title":"InteractiveClassifier"},{"location":"api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.fit","text":"Show source code in classification/interactiveclassifier.py 114 115 116 117 118 119 120 def fit ( self , X , y ): \"\"\" Fit the classifier. Bit of a formality, it's not doing anything specifically. \"\"\" self . classes_ = list ( self . json_desc [ 0 ][ \"polygons\" ] . keys ()) self . fitted_ = True return self Fit the classifier. Bit of a formality, it's not doing anything specifically.","title":"fit()"},{"location":"api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.from_json","text":"Show source code in classification/interactiveclassifier.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @classmethod def from_json ( cls , path , smoothing = 0.001 , refit = True ): \"\"\" Load the classifier from json stored on disk. Arguments: path: path of the json file smoothing: smoothing to apply to poly-counts refit: if `True`, you no longer need to call `.fit(X, y)` in order to `.predict(X)` Usage: ```python from hulearn.classification import InteractiveClassifier InteractiveClassifier.from_json(\"path/to/file.json\") ``` \"\"\" json_desc = json . loads ( pathlib . Path ( path ) . read_text ()) return InteractiveClassifier ( json_desc = json_desc , smoothing = smoothing , refit = refit ) Load the classifier from json stored on disk. Parameters Name Type Description Default path path of the json file required smoothing smoothing to apply to poly-counts 0.001 refit if True , you no longer need to call .fit(X, y) in order to .predict(X) True Usage: from hulearn.classification import InteractiveClassifier InteractiveClassifier . from_json ( \"path/to/file.json\" )","title":"from_json()"},{"location":"api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.predict","text":"Show source code in classification/interactiveclassifier.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def predict ( self , X ): \"\"\" Predicts the class for each item in `X`. Usage: ```python from hulearn.classification import InteractiveClassifier clf = InteractiveClassifier(clf_data) X, y = load_data(...) # This doesn't do anything. But scikit-learn demands it. clf.fit(X, y) # This makes predictions, based on your drawn model. clf.predict(X) ``` \"\"\" check_is_fitted ( self , [ \"classes_\" , \"fitted_\" ]) return np . array ( [ self . classes_ [ i ] for i in self . predict_proba ( X ) . argmax ( axis = 1 )] ) Predicts the class for each item in X . Usage: from hulearn.classification import InteractiveClassifier clf = InteractiveClassifier ( clf_data ) X , y = load_data ( ... ) # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. clf . predict ( X )","title":"predict()"},{"location":"api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.predict_proba","text":"Show source code in classification/interactiveclassifier.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def predict_proba ( self , X ): \"\"\" Predicts the associated probabilities for each class. Usage: ```python from hulearn.classification import InteractiveClassifier clf = InteractiveClassifier(clf_data) X, y = load_data(...) # This doesn't do anything. But scikit-learn demands it. clf.fit(X, y) # This makes predictions, based on your drawn model. clf.predict_proba(X) ``` \"\"\" # Because we're not doing anything during training, for convenience this # method can formally \"fit\" during the predict call. This is a scikit-learn # anti-pattern so we allow you to turn this off. if self . refit : if not self . fitted_ : self . fit ( X ) check_is_fitted ( self , [ \"classes_\" , \"fitted_\" ]) if isinstance ( X , pd . DataFrame ): hits = [ self . _count_hits ( self . poly_data , x [ 1 ] . to_dict ()) for x in X . iterrows () ] else : hits = [ self . _count_hits ( self . poly_data , { k : v for k , v in enumerate ( x )}) for x in X ] count_arr = ( np . array ([[ h [ c ] for c in self . classes_ ] for h in hits ]) + self . smoothing ) return count_arr / count_arr . sum ( axis = 1 ) . reshape ( - 1 , 1 ) Predicts the associated probabilities for each class. Usage: from hulearn.classification import InteractiveClassifier clf = InteractiveClassifier ( clf_data ) X , y = load_data ( ... ) # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. clf . predict_proba ( X )","title":"predict_proba()"},{"location":"api/common.html","text":"from hulearn.common import * \u00b6 df_to_dictlist ( dataf ) \u00b6 Show source code in hulearn/common.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def df_to_dictlist ( dataf ): \"\"\" Helper function, takes a dataframe and turns it into a list of dictionaries. This might make it easier to write if else chains in `FunctionClassifier`. Usage: ```python import pandas as pd from hulearn.common import df_to_dictlist df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) res = df_to_dictlist(df) assert res == [{\"a\": 1, \"b\": 4}, {\"a\": 2, \"b\": 5}, {\"a\": 3, \"b\": 6}] ``` \"\"\" data = dataf . iterrows () return [ dict ( d ) for i , d in data ] Helper function, takes a dataframe and turns it into a list of dictionaries. This might make it easier to write if else chains in FunctionClassifier . Usage: import pandas as pd from hulearn.common import df_to_dictlist df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) res = df_to_dictlist ( df ) assert res == [{ \"a\" : 1 , \"b\" : 4 }, { \"a\" : 2 , \"b\" : 5 }, { \"a\" : 3 , \"b\" : 6 }] flatten ( nested_iterable ) \u00b6 Show source code in hulearn/common.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def flatten ( nested_iterable ): \"\"\" Helper function, returns an iterator of flattened values from an arbitrarily nested iterable. Usage: ```python from hulearn.common import flatten res1 = list(flatten([['test1', 'test2'], ['a', 'b', ['c', 'd']]])) res2 = list(flatten(['test1', ['test2']])) assert res1 == ['test1', 'test2', 'a', 'b', 'c', 'd'] assert res2 == ['test1', 'test2'] ``` \"\"\" for el in nested_iterable : if isinstance ( el , collections . abc . Iterable ) and not isinstance ( el , ( str , bytes ) ): yield from flatten ( el ) else : yield el Helper function, returns an iterator of flattened values from an arbitrarily nested iterable. Usage: from hulearn.common import flatten res1 = list ( flatten ([[ 'test1' , 'test2' ], [ 'a' , 'b' , [ 'c' , 'd' ]]])) res2 = list ( flatten ([ 'test1' , [ 'test2' ]])) assert res1 == [ 'test1' , 'test2' , 'a' , 'b' , 'c' , 'd' ] assert res2 == [ 'test1' , 'test2' ]","title":"Common"},{"location":"api/common.html#from-hulearncommon-import","text":"","title":"from hulearn.common import *"},{"location":"api/common.html#hulearn.common.df_to_dictlist","text":"Show source code in hulearn/common.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def df_to_dictlist ( dataf ): \"\"\" Helper function, takes a dataframe and turns it into a list of dictionaries. This might make it easier to write if else chains in `FunctionClassifier`. Usage: ```python import pandas as pd from hulearn.common import df_to_dictlist df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) res = df_to_dictlist(df) assert res == [{\"a\": 1, \"b\": 4}, {\"a\": 2, \"b\": 5}, {\"a\": 3, \"b\": 6}] ``` \"\"\" data = dataf . iterrows () return [ dict ( d ) for i , d in data ] Helper function, takes a dataframe and turns it into a list of dictionaries. This might make it easier to write if else chains in FunctionClassifier . Usage: import pandas as pd from hulearn.common import df_to_dictlist df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) res = df_to_dictlist ( df ) assert res == [{ \"a\" : 1 , \"b\" : 4 }, { \"a\" : 2 , \"b\" : 5 }, { \"a\" : 3 , \"b\" : 6 }]","title":"df_to_dictlist()"},{"location":"api/common.html#hulearn.common.flatten","text":"Show source code in hulearn/common.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def flatten ( nested_iterable ): \"\"\" Helper function, returns an iterator of flattened values from an arbitrarily nested iterable. Usage: ```python from hulearn.common import flatten res1 = list(flatten([['test1', 'test2'], ['a', 'b', ['c', 'd']]])) res2 = list(flatten(['test1', ['test2']])) assert res1 == ['test1', 'test2', 'a', 'b', 'c', 'd'] assert res2 == ['test1', 'test2'] ``` \"\"\" for el in nested_iterable : if isinstance ( el , collections . abc . Iterable ) and not isinstance ( el , ( str , bytes ) ): yield from flatten ( el ) else : yield el Helper function, returns an iterator of flattened values from an arbitrarily nested iterable. Usage: from hulearn.common import flatten res1 = list ( flatten ([[ 'test1' , 'test2' ], [ 'a' , 'b' , [ 'c' , 'd' ]]])) res2 = list ( flatten ([ 'test1' , [ 'test2' ]])) assert res1 == [ 'test1' , 'test2' , 'a' , 'b' , 'c' , 'd' ] assert res2 == [ 'test1' , 'test2' ]","title":"flatten()"},{"location":"api/datasets.html","text":"from hulearn.datasets import * \u00b6 load_fish ( return_X_y = False , as_frame = False ) \u00b6 Show source code in hulearn/datasets.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def load_fish ( return_X_y : bool = False , as_frame : bool = False ): \"\"\" Loads in a subset of the Fish market dataset. You can find the full dataset [here](https://www.kaggle.com/aungpyaeap/fish-market). Arguments: return_X_y: return a tuple of (`X`, `y`) for convenience as_frame: return all the data as a pandas dataframe Usage: ```python from hulearn.datasets import load_fish df = load_fish(as_frame=True) X, y = load_fish(return_X_y=True) ``` \"\"\" filepath = resource_filename ( \"hulearn\" , os . path . join ( \"data\" , \"fish.zip\" )) df = pd . read_csv ( filepath ) if as_frame : return df X , y = ( df [[ \"Species\" , \"Length1\" , \"Length2\" , \"Length3\" , \"Height\" , \"Width\" ]] . values , df [ \"Weight\" ] . values , ) if return_X_y : return X , y return { \"data\" : X , \"target\" : y } Loads in a subset of the Fish market dataset. You can find the full dataset here . Parameters Name Type Description Default return_X_y bool return a tuple of ( X , y ) for convenience False as_frame bool return all the data as a pandas dataframe False Usage: from hulearn.datasets import load_fish df = load_fish ( as_frame = True ) X , y = load_fish ( return_X_y = True ) load_titanic ( return_X_y = False , as_frame = False ) \u00b6 Show source code in hulearn/datasets.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def load_titanic ( return_X_y : bool = False , as_frame : bool = False ): \"\"\" Loads in a subset of the titanic dataset. You can find the full dataset [here](https://www.kaggle.com/c/titanic/data). Arguments: return_X_y: return a tuple of (`X`, `y`) for convenience as_frame: return all the data as a pandas dataframe Usage: ```python from hulearn.datasets import load_titanic df = load_titanic(as_frame=True) X, y = load_titanic(return_X_y=True) ``` \"\"\" filepath = resource_filename ( \"hulearn\" , os . path . join ( \"data\" , \"titanic.zip\" )) df = pd . read_csv ( filepath ) if as_frame : return df X , y = ( df [[ \"pclass\" , \"name\" , \"sex\" , \"age\" , \"fare\" , \"sibsp\" , \"parch\" ]] . values , df [ \"survived\" ] . values , ) if return_X_y : return X , y return { \"data\" : X , \"target\" : y } Loads in a subset of the titanic dataset. You can find the full dataset here . Parameters Name Type Description Default return_X_y bool return a tuple of ( X , y ) for convenience False as_frame bool return all the data as a pandas dataframe False Usage: from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = load_titanic ( return_X_y = True )","title":"Datasets"},{"location":"api/datasets.html#from-hulearndatasets-import","text":"","title":"from hulearn.datasets import *"},{"location":"api/datasets.html#hulearn.datasets.load_fish","text":"Show source code in hulearn/datasets.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def load_fish ( return_X_y : bool = False , as_frame : bool = False ): \"\"\" Loads in a subset of the Fish market dataset. You can find the full dataset [here](https://www.kaggle.com/aungpyaeap/fish-market). Arguments: return_X_y: return a tuple of (`X`, `y`) for convenience as_frame: return all the data as a pandas dataframe Usage: ```python from hulearn.datasets import load_fish df = load_fish(as_frame=True) X, y = load_fish(return_X_y=True) ``` \"\"\" filepath = resource_filename ( \"hulearn\" , os . path . join ( \"data\" , \"fish.zip\" )) df = pd . read_csv ( filepath ) if as_frame : return df X , y = ( df [[ \"Species\" , \"Length1\" , \"Length2\" , \"Length3\" , \"Height\" , \"Width\" ]] . values , df [ \"Weight\" ] . values , ) if return_X_y : return X , y return { \"data\" : X , \"target\" : y } Loads in a subset of the Fish market dataset. You can find the full dataset here . Parameters Name Type Description Default return_X_y bool return a tuple of ( X , y ) for convenience False as_frame bool return all the data as a pandas dataframe False Usage: from hulearn.datasets import load_fish df = load_fish ( as_frame = True ) X , y = load_fish ( return_X_y = True )","title":"load_fish()"},{"location":"api/datasets.html#hulearn.datasets.load_titanic","text":"Show source code in hulearn/datasets.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def load_titanic ( return_X_y : bool = False , as_frame : bool = False ): \"\"\" Loads in a subset of the titanic dataset. You can find the full dataset [here](https://www.kaggle.com/c/titanic/data). Arguments: return_X_y: return a tuple of (`X`, `y`) for convenience as_frame: return all the data as a pandas dataframe Usage: ```python from hulearn.datasets import load_titanic df = load_titanic(as_frame=True) X, y = load_titanic(return_X_y=True) ``` \"\"\" filepath = resource_filename ( \"hulearn\" , os . path . join ( \"data\" , \"titanic.zip\" )) df = pd . read_csv ( filepath ) if as_frame : return df X , y = ( df [[ \"pclass\" , \"name\" , \"sex\" , \"age\" , \"fare\" , \"sibsp\" , \"parch\" ]] . values , df [ \"survived\" ] . values , ) if return_X_y : return X , y return { \"data\" : X , \"target\" : y } Loads in a subset of the titanic dataset. You can find the full dataset here . Parameters Name Type Description Default return_X_y bool return a tuple of ( X , y ) for convenience False as_frame bool return all the data as a pandas dataframe False Usage: from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = load_titanic ( return_X_y = True )","title":"load_titanic()"},{"location":"api/interactive-charts.html","text":"InteractiveCharts \u00b6 This tool allows you to interactively \"draw\" a model. Parameters Name Type Description Default dataf the dataframe to make a single interactive chart for required labels the labels to be drawn, if str we assume a column from the dataframe is chosen, if list we required color you can manually override the color of the dots to be determined by a column in a dataframe. This setting is useful when you want to input a list of labels but still want to color the dots based on a column value. None Usage: from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) charts = InteractiveCharts ( df , labels = \"species\" ) add_chart ( self , x , y , size = 5 , alpha = 0.5 , width = 400 , height = 400 , legend = True ) \u00b6 Show source code in experimental/interactive.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def add_chart ( self , x , y , size = 5 , alpha = 0.5 , width = 400 , height = 400 , legend = True ): \"\"\" Generate an interactive chart to a cell. The supported actions include: - Add patch or multi-line: Double tap to add the first vertex, then use tap to add each subsequent vertex, to finalize the draw action double tap to insert the final vertex or press the <<esc> key. - Move patch or ulti-line: Tap and drag an existing patch/multi-line, the point will be dropped once you let go of the mouse button. - Delete patch or multi-line: Tap a patch/multi-line to select it then press <<backspace>> key while the mouse is within the plot area. Arguments: x: the column from the dataset to place on the x-axis y: the column from the dataset to place on the y-axis size: the size of the drawn points alpha: the alpha (see-through-ness) of the drawn points width: the width of the chart height: the height of the chart legend: show a legend as well Usage: ```python from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins(as_frame=True) charts = InteractiveCharts(df, labels=\"species\") # Next notebook cell charts.add_chart(x=\"bill_length_mm\", y=\"bill_depth_mm\") # Next notebook cell charts.add_chart(x=\"flipper_length_mm\", y=\"body_mass_g\") # After drawing a model, export the data json_data = charts.data() ``` \"\"\" chart = SingleInteractiveChart ( dataf = self . dataf . copy (), labels = self . labels , x = x , y = y , size = size , alpha = alpha , width = width , height = height , color = self . color , legend = legend , ) self . charts . append ( chart ) chart . show () Generate an interactive chart to a cell. The supported actions include: Add patch or multi-line: Double tap to add the first vertex, then use tap to add each subsequent vertex, to finalize the draw action double tap to insert the final vertex or press the < key. Move patch or ulti-line: Tap and drag an existing patch/multi-line, the point will be dropped once you let go of the mouse button. Delete patch or multi-line: Tap a patch/multi-line to select it then press < > key while the mouse is within the plot area. Parameters Name Type Description Default x the column from the dataset to place on the x-axis required y the column from the dataset to place on the y-axis required size the size of the drawn points 5 alpha the alpha (see-through-ness) of the drawn points 0.5 width the width of the chart 400 height the height of the chart 400 legend show a legend as well True Usage: from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) charts = InteractiveCharts ( df , labels = \"species\" ) # Next notebook cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Next notebook cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) # After drawing a model, export the data json_data = charts . data () parallel_coordinates \u00b6 Creates an interactive parallel coordinates chart to help with classification tasks. Parameters Name Type Description Default dataf the dataframe to render required label the column that represents the label, will be used for coloring required height the height of the chart, in pixels 200 Usage: from hulearn.datasets import load_titanic from hulearn.experimental.interactive import parallel_coordinates df = load_titanic ( as_frame = True ) parallel_coordinates ( df , label = \"survived\" , height = 200 )","title":"Charts"},{"location":"api/interactive-charts.html#interactivecharts","text":"This tool allows you to interactively \"draw\" a model. Parameters Name Type Description Default dataf the dataframe to make a single interactive chart for required labels the labels to be drawn, if str we assume a column from the dataframe is chosen, if list we required color you can manually override the color of the dots to be determined by a column in a dataframe. This setting is useful when you want to input a list of labels but still want to color the dots based on a column value. None Usage: from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) charts = InteractiveCharts ( df , labels = \"species\" )","title":"InteractiveCharts"},{"location":"api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.add_chart","text":"Show source code in experimental/interactive.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def add_chart ( self , x , y , size = 5 , alpha = 0.5 , width = 400 , height = 400 , legend = True ): \"\"\" Generate an interactive chart to a cell. The supported actions include: - Add patch or multi-line: Double tap to add the first vertex, then use tap to add each subsequent vertex, to finalize the draw action double tap to insert the final vertex or press the <<esc> key. - Move patch or ulti-line: Tap and drag an existing patch/multi-line, the point will be dropped once you let go of the mouse button. - Delete patch or multi-line: Tap a patch/multi-line to select it then press <<backspace>> key while the mouse is within the plot area. Arguments: x: the column from the dataset to place on the x-axis y: the column from the dataset to place on the y-axis size: the size of the drawn points alpha: the alpha (see-through-ness) of the drawn points width: the width of the chart height: the height of the chart legend: show a legend as well Usage: ```python from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins(as_frame=True) charts = InteractiveCharts(df, labels=\"species\") # Next notebook cell charts.add_chart(x=\"bill_length_mm\", y=\"bill_depth_mm\") # Next notebook cell charts.add_chart(x=\"flipper_length_mm\", y=\"body_mass_g\") # After drawing a model, export the data json_data = charts.data() ``` \"\"\" chart = SingleInteractiveChart ( dataf = self . dataf . copy (), labels = self . labels , x = x , y = y , size = size , alpha = alpha , width = width , height = height , color = self . color , legend = legend , ) self . charts . append ( chart ) chart . show () Generate an interactive chart to a cell. The supported actions include: Add patch or multi-line: Double tap to add the first vertex, then use tap to add each subsequent vertex, to finalize the draw action double tap to insert the final vertex or press the < key. Move patch or ulti-line: Tap and drag an existing patch/multi-line, the point will be dropped once you let go of the mouse button. Delete patch or multi-line: Tap a patch/multi-line to select it then press < > key while the mouse is within the plot area. Parameters Name Type Description Default x the column from the dataset to place on the x-axis required y the column from the dataset to place on the y-axis required size the size of the drawn points 5 alpha the alpha (see-through-ness) of the drawn points 0.5 width the width of the chart 400 height the height of the chart 400 legend show a legend as well True Usage: from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) charts = InteractiveCharts ( df , labels = \"species\" ) # Next notebook cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Next notebook cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) # After drawing a model, export the data json_data = charts . data ()","title":"add_chart()"},{"location":"api/interactive-charts.html#parallel_coordinates","text":"Creates an interactive parallel coordinates chart to help with classification tasks. Parameters Name Type Description Default dataf the dataframe to render required label the column that represents the label, will be used for coloring required height the height of the chart, in pixels 200 Usage: from hulearn.datasets import load_titanic from hulearn.experimental.interactive import parallel_coordinates df = load_titanic ( as_frame = True ) parallel_coordinates ( df , label = \"survived\" , height = 200 )","title":"parallel_coordinates"},{"location":"api/outlier.html","text":"from hulearn.outlier import * \u00b6 FunctionOutlierDetector \u00b6 This class allows you to pass a function to detect outliers you're interested in. Note that the output of the function needs to be an array with [-1, 1] values (-1 denotes outliers). Parameters Name Type Description Default func the function that return an array of True/False required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! fit ( self , X , y = None ) \u00b6 Show source code in outlier/functionoutlier.py 21 22 23 24 25 26 27 28 def fit ( self , X , y = None ): \"\"\" Fit the classifier. No-Op. \"\"\" # Run it to confirm no error happened. self . fitted_ = True _ = self . func ( X , ** self . kwargs ) return self Fit the classifier. No-Op. partial_fit ( self , X , y = None ) \u00b6 Show source code in outlier/functionoutlier.py 30 31 32 33 34 35 36 37 38 def partial_fit ( self , X , y = None ): \"\"\" Fit the classifier partially. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True self . ncol_ = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] return self Fit the classifier partially. No-Op. predict ( self , X ) \u00b6 Show source code in outlier/functionoutlier.py 40 41 42 43 44 45 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function. InteractiveOutlierDetector \u00b6 This tool allows you to take a drawn model and use it as an outlier detector. If a datapoint does not fit in any of the drawn polygons it becomes a candidate to become an outlier. Parameters Name Type Description Default json_desc python dictionary that contains drawn data required threshold the minimum number of polygons a point needs to be in to not be considered an outlier 1 Usage: from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) charts = InteractiveCharts ( df , labels = \"species\" ) # Next notebook cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Next notebook cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) # After drawing a model, export the data json_data = charts . data () # You can now use your drawn intuition as a model! from hulearn.outlier import InteractiveOutlierDetector clf = InteractiveOutlierDetector ( clf_data ) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. # It can also be used in `GridSearchCV` for benchmarking! clf . predict ( X ) fit ( self , X , y = None ) \u00b6 Show source code in outlier/interactiveoutlier.py 105 106 107 108 109 110 def fit ( self , X , y = None ): \"\"\" Fit the classifier. Bit of a formality, it's not doing anything specifically. \"\"\" self . classes_ = list ( self . json_desc [ 0 ][ \"polygons\" ] . keys ()) return self Fit the classifier. Bit of a formality, it's not doing anything specifically. from_json ( path , threshold = 1 ) (classmethod) \u00b6 Show source code in outlier/interactiveoutlier.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @classmethod def from_json ( cls , path , threshold = 1 ): \"\"\" Load the classifier from json stored on disk. Arguments: path: path of the json file threshold: the minimum number of polygons a point needs to be in to not be considered an outlier Usage: ```python from hulearn.outlier import InteractiveOutlierDetector InteractiveOutlierDetector.from_json(\"path/to/file.json\") ``` \"\"\" json_desc = json . loads ( pathlib . Path ( path ) . read_text ()) return InteractiveOutlierDetector ( json_desc = json_desc , threshold = threshold ) Load the classifier from json stored on disk. Parameters Name Type Description Default path path of the json file required threshold the minimum number of polygons a point needs to be in to not be considered an outlier 1 Usage: from hulearn.outlier import InteractiveOutlierDetector InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) predict ( self , X ) \u00b6 Show source code in outlier/interactiveoutlier.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def predict ( self , X ): \"\"\" Predicts the associated probabilities for each class. Usage: ```python from hulearn.drawing-classifier.interactive import InteractiveOutlierDetector # Assuming a variable `clf_data` that contains the drawn polygons. clf = InteractiveOutlierDetector(clf_data) X, y = load_data(...) # This doesn't do anything. But scikit-learn demands it. clf.fit(X, y) # This makes predictions, based on your drawn model. clf.predict_proba(X) ``` \"\"\" count_arr = self . score ( X ) return np . where ( count_arr . sum ( axis = 1 ) < self . threshold , - 1 , 1 ) Predicts the associated probabilities for each class. Usage: from hulearn.drawing - classifier . interactive import InteractiveOutlierDetector # Assuming a variable `clf_data` that contains the drawn polygons. clf = InteractiveOutlierDetector ( clf_data ) X , y = load_data ( ... ) # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. clf . predict_proba ( X )","title":"Outlier"},{"location":"api/outlier.html#from-hulearnoutlier-import","text":"","title":"from hulearn.outlier import *"},{"location":"api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector","text":"This class allows you to pass a function to detect outliers you're interested in. Note that the output of the function needs to be an array with [-1, 1] values (-1 denotes outliers). Parameters Name Type Description Default func the function that return an array of True/False required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions!","title":"FunctionOutlierDetector"},{"location":"api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.fit","text":"Show source code in outlier/functionoutlier.py 21 22 23 24 25 26 27 28 def fit ( self , X , y = None ): \"\"\" Fit the classifier. No-Op. \"\"\" # Run it to confirm no error happened. self . fitted_ = True _ = self . func ( X , ** self . kwargs ) return self Fit the classifier. No-Op.","title":"fit()"},{"location":"api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.partial_fit","text":"Show source code in outlier/functionoutlier.py 30 31 32 33 34 35 36 37 38 def partial_fit ( self , X , y = None ): \"\"\" Fit the classifier partially. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True self . ncol_ = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] return self Fit the classifier partially. No-Op.","title":"partial_fit()"},{"location":"api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.predict","text":"Show source code in outlier/functionoutlier.py 40 41 42 43 44 45 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"predict()"},{"location":"api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector","text":"This tool allows you to take a drawn model and use it as an outlier detector. If a datapoint does not fit in any of the drawn polygons it becomes a candidate to become an outlier. Parameters Name Type Description Default json_desc python dictionary that contains drawn data required threshold the minimum number of polygons a point needs to be in to not be considered an outlier 1 Usage: from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) charts = InteractiveCharts ( df , labels = \"species\" ) # Next notebook cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Next notebook cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) # After drawing a model, export the data json_data = charts . data () # You can now use your drawn intuition as a model! from hulearn.outlier import InteractiveOutlierDetector clf = InteractiveOutlierDetector ( clf_data ) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. # It can also be used in `GridSearchCV` for benchmarking! clf . predict ( X )","title":"InteractiveOutlierDetector"},{"location":"api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.fit","text":"Show source code in outlier/interactiveoutlier.py 105 106 107 108 109 110 def fit ( self , X , y = None ): \"\"\" Fit the classifier. Bit of a formality, it's not doing anything specifically. \"\"\" self . classes_ = list ( self . json_desc [ 0 ][ \"polygons\" ] . keys ()) return self Fit the classifier. Bit of a formality, it's not doing anything specifically.","title":"fit()"},{"location":"api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.from_json","text":"Show source code in outlier/interactiveoutlier.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @classmethod def from_json ( cls , path , threshold = 1 ): \"\"\" Load the classifier from json stored on disk. Arguments: path: path of the json file threshold: the minimum number of polygons a point needs to be in to not be considered an outlier Usage: ```python from hulearn.outlier import InteractiveOutlierDetector InteractiveOutlierDetector.from_json(\"path/to/file.json\") ``` \"\"\" json_desc = json . loads ( pathlib . Path ( path ) . read_text ()) return InteractiveOutlierDetector ( json_desc = json_desc , threshold = threshold ) Load the classifier from json stored on disk. Parameters Name Type Description Default path path of the json file required threshold the minimum number of polygons a point needs to be in to not be considered an outlier 1 Usage: from hulearn.outlier import InteractiveOutlierDetector InteractiveOutlierDetector . from_json ( \"path/to/file.json\" )","title":"from_json()"},{"location":"api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.predict","text":"Show source code in outlier/interactiveoutlier.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def predict ( self , X ): \"\"\" Predicts the associated probabilities for each class. Usage: ```python from hulearn.drawing-classifier.interactive import InteractiveOutlierDetector # Assuming a variable `clf_data` that contains the drawn polygons. clf = InteractiveOutlierDetector(clf_data) X, y = load_data(...) # This doesn't do anything. But scikit-learn demands it. clf.fit(X, y) # This makes predictions, based on your drawn model. clf.predict_proba(X) ``` \"\"\" count_arr = self . score ( X ) return np . where ( count_arr . sum ( axis = 1 ) < self . threshold , - 1 , 1 ) Predicts the associated probabilities for each class. Usage: from hulearn.drawing - classifier . interactive import InteractiveOutlierDetector # Assuming a variable `clf_data` that contains the drawn polygons. clf = InteractiveOutlierDetector ( clf_data ) X , y = load_data ( ... ) # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. clf . predict_proba ( X )","title":"predict()"},{"location":"api/preprocessing.html","text":"from hulearn.preprocessing import * \u00b6 PipeTransformer \u00b6 This transformer allows you to define a function that will take in data and transform it however you like. You can specify keyword arguments that you can benchmark as well. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! Usage: import pandas as pd from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV from hulearn.datasets import load_titanic from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () # I'm not using .assign() in this pipeline because lambda functions # do not pickle and GridSearchCV demands that it can. if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] pipe = Pipeline ([ ( 'prep' , PipeTransformer ( preprocessing , n_char = True , gender = True )), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) pd . DataFrame ( grid . cv_results_ )[[ 'param_prep__gender' , 'param_prep__n_char' , 'mean_test_score' ]] fit ( self , X , y = None ) \u00b6 Show source code in preprocessing/pipetransformer.py 62 63 64 65 66 67 68 69 70 def fit ( self , X , y = None ): \"\"\" Fit the classifier. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True self . ncol_ = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] return self Fit the classifier. No-Op. partial_fit ( self , X , y = None ) \u00b6 Show source code in preprocessing/pipetransformer.py 72 73 74 75 76 77 78 79 80 def partial_fit ( self , X , y = None ): \"\"\" Fit the classifier partially. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True self . ncol_ = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] return self Fit the classifier partially. No-Op. transform ( self , X ) \u00b6 Show source code in preprocessing/pipetransformer.py 82 83 84 85 86 87 88 89 90 91 92 def transform ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" , \"ncol_\" ]) ncol = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] if self . ncol_ != ncol : raise ValueError ( f \"Reshape your data, there were { self . ncol_ } features during training, now= { ncol } .\" ) return self . func ( X , ** self . kwargs ) Make predictions using the passed function. InteractivePreprocessor \u00b6 This tool allows you to take a drawn model and use it as a featurizer. Parameters Name Type Description Default json_desc chart da ta in dictionary form required refit if True , you no longer need to call .fit(X, y) in order to .predict(X) True fit ( self , X , y = None ) \u00b6 Show source code in preprocessing/interactivepreprocessor.py 75 76 77 78 79 80 81 def fit ( self , X , y = None ): \"\"\" Fit the classifier. Bit of a formality, it's not doing anything specifically. \"\"\" self . classes_ = list ( self . json_desc [ 0 ][ \"polygons\" ] . keys ()) self . fitted_ = True return self Fit the classifier. Bit of a formality, it's not doing anything specifically. from_json ( path , refit = True ) (classmethod) \u00b6 Show source code in preprocessing/interactivepreprocessor.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 @classmethod def from_json ( cls , path , refit = True ): \"\"\" Load the classifier from json stored on disk. Arguments: path: path of the json file refit: if `True`, you no longer need to call `.fit(X, y)` in order to `.predict(X)` Usage: ```python from hulearn.classification import InteractivePreprocessor InteractivePreprocessor.from_json(\"path/to/file.json\") ``` \"\"\" json_desc = json . loads ( pathlib . Path ( path ) . read_text ()) return InteractivePreprocessor ( json_desc = json_desc , refit = refit ) Load the classifier from json stored on disk. Parameters Name Type Description Default path path of the json file required refit if True , you no longer need to call .fit(X, y) in order to .predict(X) True Usage: from hulearn.classification import InteractivePreprocessor InteractivePreprocessor . from_json ( \"path/to/file.json\" ) pandas_pipe ( self , dataf ) \u00b6 Show source code in preprocessing/interactivepreprocessor.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def pandas_pipe ( self , dataf ): \"\"\" Use this transformer as part of a `.pipe()` method chain in pandas. Usage: ```python import numpy as np import pandas as pd # Load in a dataframe from somewhere df = load_data(...) # Load in drawn chart data from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor.from_json(\"path/file.json\") # This adds new columns to the dataframe df.pipe(pandas_pipe) ``` \"\"\" new_dataf = pd . DataFrame ( self . fit ( dataf ) . transform ( dataf ), columns = self . classes_ ) return pd . concat ( [ dataf . copy () . reset_index ( drop = True ), new_dataf . reset_index ( drop = True )], axis = 1 , ) Use this transformer as part of a .pipe() method chain in pandas. Usage: import numpy as np import pandas as pd # Load in a dataframe from somewhere df = load_data ( ... ) # Load in drawn chart data from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor . from_json ( \"path/file.json\" ) # This adds new columns to the dataframe df . pipe ( pandas_pipe ) transform ( self , X ) \u00b6 Show source code in preprocessing/interactivepreprocessor.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def transform ( self , X ): \"\"\" Apply the counting/binning based on the drawings. Usage: ```python from hulearn.preprocessing import InteractivePreprocessor clf = InteractivePreprocessor(clf_data) X, y = load_data(...) # This doesn't do anything. But scikit-learn demands it. clf.fit(X, y) # This makes predictions, based on your drawn model. clf.transform(X) ``` \"\"\" # Because we're not doing anything during training, for convenience this # method can formally \"fit\" during the predict call. This is a scikit-learn # anti-pattern so we allow you to turn this off. if self . refit : if not self . fitted_ : self . fit ( X ) check_is_fitted ( self , [ \"classes_\" , \"fitted_\" ]) if isinstance ( X , pd . DataFrame ): hits = [ self . _count_hits ( self . poly_data , x [ 1 ] . to_dict ()) for x in X . iterrows () ] else : hits = [ self . _count_hits ( self . poly_data , { k : v for k , v in enumerate ( x )}) for x in X ] count_arr = np . array ([[ h [ c ] for c in self . classes_ ] for h in hits ]) return count_arr Apply the counting/binning based on the drawings. Usage: from hulearn.preprocessing import InteractivePreprocessor clf = InteractivePreprocessor ( clf_data ) X , y = load_data ( ... ) # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. clf . transform ( X )","title":"Preprocessing"},{"location":"api/preprocessing.html#from-hulearnpreprocessing-import","text":"","title":"from hulearn.preprocessing import *"},{"location":"api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer","text":"This transformer allows you to define a function that will take in data and transform it however you like. You can specify keyword arguments that you can benchmark as well. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! Usage: import pandas as pd from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV from hulearn.datasets import load_titanic from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () # I'm not using .assign() in this pipeline because lambda functions # do not pickle and GridSearchCV demands that it can. if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] pipe = Pipeline ([ ( 'prep' , PipeTransformer ( preprocessing , n_char = True , gender = True )), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) pd . DataFrame ( grid . cv_results_ )[[ 'param_prep__gender' , 'param_prep__n_char' , 'mean_test_score' ]]","title":"PipeTransformer"},{"location":"api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.fit","text":"Show source code in preprocessing/pipetransformer.py 62 63 64 65 66 67 68 69 70 def fit ( self , X , y = None ): \"\"\" Fit the classifier. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True self . ncol_ = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] return self Fit the classifier. No-Op.","title":"fit()"},{"location":"api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.partial_fit","text":"Show source code in preprocessing/pipetransformer.py 72 73 74 75 76 77 78 79 80 def partial_fit ( self , X , y = None ): \"\"\" Fit the classifier partially. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True self . ncol_ = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] return self Fit the classifier partially. No-Op.","title":"partial_fit()"},{"location":"api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.transform","text":"Show source code in preprocessing/pipetransformer.py 82 83 84 85 86 87 88 89 90 91 92 def transform ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" , \"ncol_\" ]) ncol = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] if self . ncol_ != ncol : raise ValueError ( f \"Reshape your data, there were { self . ncol_ } features during training, now= { ncol } .\" ) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"transform()"},{"location":"api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor","text":"This tool allows you to take a drawn model and use it as a featurizer. Parameters Name Type Description Default json_desc chart da ta in dictionary form required refit if True , you no longer need to call .fit(X, y) in order to .predict(X) True","title":"InteractivePreprocessor"},{"location":"api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.fit","text":"Show source code in preprocessing/interactivepreprocessor.py 75 76 77 78 79 80 81 def fit ( self , X , y = None ): \"\"\" Fit the classifier. Bit of a formality, it's not doing anything specifically. \"\"\" self . classes_ = list ( self . json_desc [ 0 ][ \"polygons\" ] . keys ()) self . fitted_ = True return self Fit the classifier. Bit of a formality, it's not doing anything specifically.","title":"fit()"},{"location":"api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.from_json","text":"Show source code in preprocessing/interactivepreprocessor.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 @classmethod def from_json ( cls , path , refit = True ): \"\"\" Load the classifier from json stored on disk. Arguments: path: path of the json file refit: if `True`, you no longer need to call `.fit(X, y)` in order to `.predict(X)` Usage: ```python from hulearn.classification import InteractivePreprocessor InteractivePreprocessor.from_json(\"path/to/file.json\") ``` \"\"\" json_desc = json . loads ( pathlib . Path ( path ) . read_text ()) return InteractivePreprocessor ( json_desc = json_desc , refit = refit ) Load the classifier from json stored on disk. Parameters Name Type Description Default path path of the json file required refit if True , you no longer need to call .fit(X, y) in order to .predict(X) True Usage: from hulearn.classification import InteractivePreprocessor InteractivePreprocessor . from_json ( \"path/to/file.json\" )","title":"from_json()"},{"location":"api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.pandas_pipe","text":"Show source code in preprocessing/interactivepreprocessor.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def pandas_pipe ( self , dataf ): \"\"\" Use this transformer as part of a `.pipe()` method chain in pandas. Usage: ```python import numpy as np import pandas as pd # Load in a dataframe from somewhere df = load_data(...) # Load in drawn chart data from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor.from_json(\"path/file.json\") # This adds new columns to the dataframe df.pipe(pandas_pipe) ``` \"\"\" new_dataf = pd . DataFrame ( self . fit ( dataf ) . transform ( dataf ), columns = self . classes_ ) return pd . concat ( [ dataf . copy () . reset_index ( drop = True ), new_dataf . reset_index ( drop = True )], axis = 1 , ) Use this transformer as part of a .pipe() method chain in pandas. Usage: import numpy as np import pandas as pd # Load in a dataframe from somewhere df = load_data ( ... ) # Load in drawn chart data from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor . from_json ( \"path/file.json\" ) # This adds new columns to the dataframe df . pipe ( pandas_pipe )","title":"pandas_pipe()"},{"location":"api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.transform","text":"Show source code in preprocessing/interactivepreprocessor.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def transform ( self , X ): \"\"\" Apply the counting/binning based on the drawings. Usage: ```python from hulearn.preprocessing import InteractivePreprocessor clf = InteractivePreprocessor(clf_data) X, y = load_data(...) # This doesn't do anything. But scikit-learn demands it. clf.fit(X, y) # This makes predictions, based on your drawn model. clf.transform(X) ``` \"\"\" # Because we're not doing anything during training, for convenience this # method can formally \"fit\" during the predict call. This is a scikit-learn # anti-pattern so we allow you to turn this off. if self . refit : if not self . fitted_ : self . fit ( X ) check_is_fitted ( self , [ \"classes_\" , \"fitted_\" ]) if isinstance ( X , pd . DataFrame ): hits = [ self . _count_hits ( self . poly_data , x [ 1 ] . to_dict ()) for x in X . iterrows () ] else : hits = [ self . _count_hits ( self . poly_data , { k : v for k , v in enumerate ( x )}) for x in X ] count_arr = np . array ([[ h [ c ] for c in self . classes_ ] for h in hits ]) return count_arr Apply the counting/binning based on the drawings. Usage: from hulearn.preprocessing import InteractivePreprocessor clf = InteractivePreprocessor ( clf_data ) X , y = load_data ( ... ) # This doesn't do anything. But scikit-learn demands it. clf . fit ( X , y ) # This makes predictions, based on your drawn model. clf . transform ( X )","title":"transform()"},{"location":"api/regression.html","text":"from hulearn.regression import * \u00b6 FunctionRegressor \u00b6 This class allows you to pass a function to make the predictions you're interested in. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! fit ( self , X , y ) \u00b6 Show source code in regression/functionregressor.py 20 21 22 23 24 25 26 27 def fit ( self , X , y ): \"\"\" Fit the classifier. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True return self Fit the classifier. No-Op. partial_fit ( self , X , y = None ) \u00b6 Show source code in regression/functionregressor.py 29 30 31 32 33 34 35 36 def partial_fit ( self , X , y = None ): \"\"\" Fit the classifier partially. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True return self Fit the classifier partially. No-Op. predict ( self , X ) \u00b6 Show source code in regression/functionregressor.py 38 39 40 41 42 43 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"Regression"},{"location":"api/regression.html#from-hulearnregression-import","text":"","title":"from hulearn.regression import *"},{"location":"api/regression.html#hulearn.regression.functionregressor.FunctionRegressor","text":"This class allows you to pass a function to make the predictions you're interested in. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions!","title":"FunctionRegressor"},{"location":"api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.fit","text":"Show source code in regression/functionregressor.py 20 21 22 23 24 25 26 27 def fit ( self , X , y ): \"\"\" Fit the classifier. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True return self Fit the classifier. No-Op.","title":"fit()"},{"location":"api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.partial_fit","text":"Show source code in regression/functionregressor.py 29 30 31 32 33 34 35 36 def partial_fit ( self , X , y = None ): \"\"\" Fit the classifier partially. No-Op. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True return self Fit the classifier partially. No-Op.","title":"partial_fit()"},{"location":"api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.predict","text":"Show source code in regression/functionregressor.py 38 39 40 41 42 43 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"predict()"},{"location":"api/rulers.html","text":"CaseWhenRuler \u00b6 Helper class to construct \"case when\"-style FunctionClassifiers. This class allows you to write a system of rules using lambda functions. These functions cannot be pickled by scikit-learn however, so if you'd like to use this class in a GridSearch you will need to wrap it around a FunctionClassifier. Parameters Name Type Description Default default the default value to predict if no rules apply None Usage: from hulearn.datasets import load_titanic from hulearn.experimental import CaseWhenRuler from hulearn.classification import FunctionClassifier def make_prediction ( dataf , age = 15 ): ruler = CaseWhenRuler ( default = 0 ) ( ruler . add_rule ( lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'sex' ] == \"female\" ), 1 , name = \"gender-rule\" ) . add_rule ( lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'age' ] <= age ), 1 , name = \"child-rule\" )) return ruler . predict ( dataf ) clf = FunctionClassifier ( make_prediction ) add_rule ( self , when , then , name = None ) \u00b6 Show source code in experimental/ruler.py 41 42 43 44 45 46 47 48 49 50 51 52 53 def add_rule ( self , when , then , name = None ): \"\"\" Adds a rule to the system. Arguments: when: a (lambda) function that tells us when the rule applies then: the value to output if the rule applies name: an optional name for the rule \"\"\" if not name : name = f \"rule- { len ( self . rules ) + 1 } \" self . rules . append (( when , then , name )) return self Adds a rule to the system. Parameters Name Type Description Default when a (lambda) function that tells us when the rule applies required then the value to output if the rule applies required name an optional name for the rule None predict ( self , X ) \u00b6 Show source code in experimental/ruler.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def predict ( self , X ): \"\"\" Makes a prediction based on the rules sofar. Usage: ```python from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction(dataf, gender_rule=True, child_rule=True, fare_rule=True): ruler = CaseWhenRuler(default=0) if gender_rule: ruler.add_rule(when=lambda d: (d['pclass'] < 3.0) & (d['sex'] == \"female\"), then=1, name=\"gender-rule\") if child_rule: ruler.add_rule(when=lambda d: (d['pclass'] < 3.0) & (d['age'] <= 15), then=1, name=\"child-rule\") if fare_rule: ruler.add_rule(when=lambda d: (d['fare'] > 100), then=1, name=\"fare-rule\") return ruler.transform(dataf) clf = FunctionClassifier(make_prediction) ``` \"\"\" results = [ self . default for x in range ( len ( X ))] for rule in self . rules : when , then , name = rule for idx , predicate in enumerate ( when ( X )): if predicate and ( results [ idx ] == self . default ): results [ idx ] = then return results Makes a prediction based on the rules sofar. Usage: from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , gender_rule = True , child_rule = True , fare_rule = True ): ruler = CaseWhenRuler ( default = 0 ) if gender_rule : ruler . add_rule ( when = lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'sex' ] == \"female\" ), then = 1 , name = \"gender-rule\" ) if child_rule : ruler . add_rule ( when = lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'age' ] <= 15 ), then = 1 , name = \"child-rule\" ) if fare_rule : ruler . add_rule ( when = lambda d : ( d [ 'fare' ] > 100 ), then = 1 , name = \"fare-rule\" ) return ruler . transform ( dataf ) clf = FunctionClassifier ( make_prediction ) transform ( self , X ) \u00b6 Show source code in experimental/ruler.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def transform ( self , X ): \"\"\" Produces a dataframe that indicates the state of all rules. Usage: ```python from hulearn.preprocessing import PipeTransformer from hulearn.experimental import CaseWhenRuler def make_prediction(dataf, gender_rule=True, child_rule=True, fare_rule=True): ruler = CaseWhenRuler(default=0) if gender_rule: ruler.add_rule(when=lambda d: (d['pclass'] < 3.0) & (d['sex'] == \"female\"), then=1, name=\"gender-rule\") if child_rule: ruler.add_rule(when=lambda d: (d['pclass'] < 3.0) & (d['age'] <= 15), then=1, name=\"child-rule\") if fare_rule: ruler.add_rule(when=lambda d: (d['fare'] > 100), then=1, name=\"fare-rule\") return ruler.transform(dataf) clf = PipeTransformer(make_prediction) ``` \"\"\" result = pd . DataFrame () for rule in self . rules : when , then , name = rule result [ name ] = when ( X ) return result Produces a dataframe that indicates the state of all rules. Usage: from hulearn.preprocessing import PipeTransformer from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , gender_rule = True , child_rule = True , fare_rule = True ): ruler = CaseWhenRuler ( default = 0 ) if gender_rule : ruler . add_rule ( when = lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'sex' ] == \"female\" ), then = 1 , name = \"gender-rule\" ) if child_rule : ruler . add_rule ( when = lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'age' ] <= 15 ), then = 1 , name = \"child-rule\" ) if fare_rule : ruler . add_rule ( when = lambda d : ( d [ 'fare' ] > 100 ), then = 1 , name = \"fare-rule\" ) return ruler . transform ( dataf ) clf = PipeTransformer ( make_prediction )","title":"Rulers"},{"location":"api/rulers.html#casewhenruler","text":"Helper class to construct \"case when\"-style FunctionClassifiers. This class allows you to write a system of rules using lambda functions. These functions cannot be pickled by scikit-learn however, so if you'd like to use this class in a GridSearch you will need to wrap it around a FunctionClassifier. Parameters Name Type Description Default default the default value to predict if no rules apply None Usage: from hulearn.datasets import load_titanic from hulearn.experimental import CaseWhenRuler from hulearn.classification import FunctionClassifier def make_prediction ( dataf , age = 15 ): ruler = CaseWhenRuler ( default = 0 ) ( ruler . add_rule ( lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'sex' ] == \"female\" ), 1 , name = \"gender-rule\" ) . add_rule ( lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'age' ] <= age ), 1 , name = \"child-rule\" )) return ruler . predict ( dataf ) clf = FunctionClassifier ( make_prediction )","title":"CaseWhenRuler"},{"location":"api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.add_rule","text":"Show source code in experimental/ruler.py 41 42 43 44 45 46 47 48 49 50 51 52 53 def add_rule ( self , when , then , name = None ): \"\"\" Adds a rule to the system. Arguments: when: a (lambda) function that tells us when the rule applies then: the value to output if the rule applies name: an optional name for the rule \"\"\" if not name : name = f \"rule- { len ( self . rules ) + 1 } \" self . rules . append (( when , then , name )) return self Adds a rule to the system. Parameters Name Type Description Default when a (lambda) function that tells us when the rule applies required then the value to output if the rule applies required name an optional name for the rule None","title":"add_rule()"},{"location":"api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.predict","text":"Show source code in experimental/ruler.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def predict ( self , X ): \"\"\" Makes a prediction based on the rules sofar. Usage: ```python from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction(dataf, gender_rule=True, child_rule=True, fare_rule=True): ruler = CaseWhenRuler(default=0) if gender_rule: ruler.add_rule(when=lambda d: (d['pclass'] < 3.0) & (d['sex'] == \"female\"), then=1, name=\"gender-rule\") if child_rule: ruler.add_rule(when=lambda d: (d['pclass'] < 3.0) & (d['age'] <= 15), then=1, name=\"child-rule\") if fare_rule: ruler.add_rule(when=lambda d: (d['fare'] > 100), then=1, name=\"fare-rule\") return ruler.transform(dataf) clf = FunctionClassifier(make_prediction) ``` \"\"\" results = [ self . default for x in range ( len ( X ))] for rule in self . rules : when , then , name = rule for idx , predicate in enumerate ( when ( X )): if predicate and ( results [ idx ] == self . default ): results [ idx ] = then return results Makes a prediction based on the rules sofar. Usage: from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , gender_rule = True , child_rule = True , fare_rule = True ): ruler = CaseWhenRuler ( default = 0 ) if gender_rule : ruler . add_rule ( when = lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'sex' ] == \"female\" ), then = 1 , name = \"gender-rule\" ) if child_rule : ruler . add_rule ( when = lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'age' ] <= 15 ), then = 1 , name = \"child-rule\" ) if fare_rule : ruler . add_rule ( when = lambda d : ( d [ 'fare' ] > 100 ), then = 1 , name = \"fare-rule\" ) return ruler . transform ( dataf ) clf = FunctionClassifier ( make_prediction )","title":"predict()"},{"location":"api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.transform","text":"Show source code in experimental/ruler.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def transform ( self , X ): \"\"\" Produces a dataframe that indicates the state of all rules. Usage: ```python from hulearn.preprocessing import PipeTransformer from hulearn.experimental import CaseWhenRuler def make_prediction(dataf, gender_rule=True, child_rule=True, fare_rule=True): ruler = CaseWhenRuler(default=0) if gender_rule: ruler.add_rule(when=lambda d: (d['pclass'] < 3.0) & (d['sex'] == \"female\"), then=1, name=\"gender-rule\") if child_rule: ruler.add_rule(when=lambda d: (d['pclass'] < 3.0) & (d['age'] <= 15), then=1, name=\"child-rule\") if fare_rule: ruler.add_rule(when=lambda d: (d['fare'] > 100), then=1, name=\"fare-rule\") return ruler.transform(dataf) clf = PipeTransformer(make_prediction) ``` \"\"\" result = pd . DataFrame () for rule in self . rules : when , then , name = rule result [ name ] = when ( X ) return result Produces a dataframe that indicates the state of all rules. Usage: from hulearn.preprocessing import PipeTransformer from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , gender_rule = True , child_rule = True , fare_rule = True ): ruler = CaseWhenRuler ( default = 0 ) if gender_rule : ruler . add_rule ( when = lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'sex' ] == \"female\" ), then = 1 , name = \"gender-rule\" ) if child_rule : ruler . add_rule ( when = lambda d : ( d [ 'pclass' ] < 3.0 ) & ( d [ 'age' ] <= 15 ), then = 1 , name = \"child-rule\" ) if fare_rule : ruler . add_rule ( when = lambda d : ( d [ 'fare' ] > 100 ), then = 1 , name = \"fare-rule\" ) return ruler . transform ( dataf ) clf = PipeTransformer ( make_prediction )","title":"transform()"},{"location":"examples/faq.html","text":"Frequently Asked Questions \u00b6 Feel free to ask questions here . What are the Lessons Learned \u00b6 If you're interested in some of the lessons the creators of this tool learned while creating it, all you need to do is follow the python tradition. from hulearn import this Why Make This? \u00b6 Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground. But we've reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This made us wonder if we could make machine learning listen more to common sense. There's a lot of things that could go wrong otherwise. If you're interested in examples we might recommend this pydata talk . I'm getting a PORT error! \u00b6 You might get an error that looks like; ERROR : bokeh . server . views . ws : Refusing websocket connection from Origin 'http://localhost:8889' ; use -- allow - websocket - origin = localhost : 8889 or set BOKEH_ALLOW_WS_ORIGIN = localhost : 8889 to permit this ; currently we allow origins { 'localhost:8888' } This is related to something bokeh cannot do without explicit permission from jupyter. It can't be fixed by this library but you can circumvent this error by running jupyter via; python -m jupyter lab --port 8889 --allow-websocket-origin=localhost:8889 You can also set an environment variable BOKEH_ALLOW_WS_ORIGIN=localhost:8889 .","title":"FAQ"},{"location":"examples/faq.html#frequently-asked-questions","text":"Feel free to ask questions here .","title":"Frequently Asked Questions"},{"location":"examples/faq.html#what-are-the-lessons-learned","text":"If you're interested in some of the lessons the creators of this tool learned while creating it, all you need to do is follow the python tradition. from hulearn import this","title":"What are the Lessons Learned"},{"location":"examples/faq.html#why-make-this","text":"Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground. But we've reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This made us wonder if we could make machine learning listen more to common sense. There's a lot of things that could go wrong otherwise. If you're interested in examples we might recommend this pydata talk .","title":"Why Make This?"},{"location":"examples/faq.html#im-getting-a-port-error","text":"You might get an error that looks like; ERROR : bokeh . server . views . ws : Refusing websocket connection from Origin 'http://localhost:8889' ; use -- allow - websocket - origin = localhost : 8889 or set BOKEH_ALLOW_WS_ORIGIN = localhost : 8889 to permit this ; currently we allow origins { 'localhost:8888' } This is related to something bokeh cannot do without explicit permission from jupyter. It can't be fixed by this library but you can circumvent this error by running jupyter via; python -m jupyter lab --port 8889 --allow-websocket-origin=localhost:8889 You can also set an environment variable BOKEH_ALLOW_WS_ORIGIN=localhost:8889 .","title":"I'm getting a PORT error!"},{"location":"examples/model-mining.html","text":"In this example, we will demonstrate that you can use visual data mining techniques to discover meaningful patterns in your data. These patterns can be easily translated into a machine learning model by using the tools found in this package. You can find a full tutorial of this technique on calmcode but the main video can be viewed below. The Task \u00b6 We're going to make a rule based model for the creditcard dataset. The main feature of the dataset is that it is suffering from a class imbalance. Instead of training a machine learning model, let's try to instead explore it with a parallel coordinates chart. If you scroll all the way to the bottom of this tutorial you'll see an example of such a chart. It shows a \"train\"-set. We explored the data just like in the video and that led us to define the following model. from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , age = 15 ): ruler = CaseWhenRuler ( default = 0 ) ( ruler . add_rule ( lambda d : ( d [ 'V11' ] > 4 ), 1 ) . add_rule ( lambda d : ( d [ 'V17' ] < - 3 ), 1 ) . add_rule ( lambda d : ( d [ 'V14' ] < - 8 ), 1 )) return ruler . predict ( dataf ) clf = FunctionClassifier ( make_prediction ) Full Code First we load the data. from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split df_credit = fetch_openml ( data_id = 1597 , as_frame = True ) credit_train , credit_test = train_test_split ( df_credit , test_size = 0.5 , shuffle = True ) Next, we create a hiplot in jupyter. import json import hiplot as hip samples = [ credit_train . loc [ lambda d : d [ 'group' ] == True ], credit_train . sample ( 5000 )] json_data = pd . concat ( samples ) . to_json ( orient = 'records' ) hip . Experiment . from_iterable ( json . loads ( json_data )) . display () Given that we have our model, we can make a classification report. from sklearn.metrics import classification_report # Note that `fit` is a no-op here. preds = clf . fit ( credit_train , credit_train [ 'group' ]) . predict ( credit_test )) print ( classification_report ( credit_test [ 'group' ], preds ) When we ran the benchmark locally, we got the following classification report. precision recall f1-score support False 1.00 1.00 1.00 142165 True 0.70 0.73 0.71 239 accuracy 1.00 142404 macro avg 0.85 0.86 0.86 142404 weighted avg 1.00 1.00 1.00 142404 Deep Learning \u00b6 It's not a perfect benchmark, but we could compare this result to the one that's demonstrated on the keras blog . The trained model there lists 86.67% precision but only 23.9% recall. Depending on your preferences for false-positives, you could argue that our model is outperforming the deep learning model. It's not 100% a fair comparison. You can imagine that the keras blogpost is written to explain keras. The auther likely didn't attempt to make a state-of-the-art model. But what this demo does show is the merit of turning an exploratory data analysis into a model. You can end up with a very interpretable model, you might learn something about your data along the way and the model might certainly still perform well. Parallel Coordinates \u00b6 If you hover of the group name and right-click, you'll be able to set it for coloring and repeat the experience in the video. By doing that it becomes quite easy to eyeball how to separate the two classes. The V17 column especially seems powerful here. In real life we might ask \"why?\" this column is so distinctive but for now we'll just play around until we find a sensible model. [hulearn.classification.functionclassifier]: /human-learn/api/classification.html#hulearn.classification.functionclassifier [hulearn.classification.functionclassifier.FunctionClassifier]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier [hulearn.classification.functionclassifier.FunctionClassifier.__init__]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.__init__ [hulearn.classification.functionclassifier.FunctionClassifier.fit]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.fit [hulearn.classification.functionclassifier.FunctionClassifier.get_params]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.get_params [hulearn.classification.functionclassifier.FunctionClassifier.partial_fit]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.partial_fit [hulearn.classification.functionclassifier.FunctionClassifier.predict]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.predict [hulearn.classification.functionclassifier.FunctionClassifier.set_params]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.set_params [hulearn.classification.interactiveclassifier]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier [hulearn.classification.interactiveclassifier.InteractiveClassifier]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier [hulearn.classification.interactiveclassifier.InteractiveClassifier.__init__]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.__init__ [hulearn.classification.interactiveclassifier.InteractiveClassifier.fit]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.fit [hulearn.classification.interactiveclassifier.InteractiveClassifier.from_json]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.from_json [hulearn.classification.interactiveclassifier.InteractiveClassifier.poly_data]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.poly_data [hulearn.classification.interactiveclassifier.InteractiveClassifier.predict]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.predict [hulearn.classification.interactiveclassifier.InteractiveClassifier.predict_proba]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.predict_proba [hulearn.regression.functionregressor]: /human-learn/api/regression.html#hulearn.regression.functionregressor [hulearn.regression.functionregressor.FunctionRegressor]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor [hulearn.regression.functionregressor.FunctionRegressor.__init__]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.__init__ [hulearn.regression.functionregressor.FunctionRegressor.fit]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.fit [hulearn.regression.functionregressor.FunctionRegressor.get_params]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.get_params [hulearn.regression.functionregressor.FunctionRegressor.partial_fit]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.partial_fit [hulearn.regression.functionregressor.FunctionRegressor.predict]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.predict [hulearn.regression.functionregressor.FunctionRegressor.set_params]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.set_params [hulearn.outlier.functionoutlier]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier [hulearn.outlier.functionoutlier.FunctionOutlierDetector]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector [hulearn.outlier.functionoutlier.FunctionOutlierDetector.__init__]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.__init__ [hulearn.outlier.functionoutlier.FunctionOutlierDetector.fit]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.fit [hulearn.outlier.functionoutlier.FunctionOutlierDetector.get_params]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.get_params [hulearn.outlier.functionoutlier.FunctionOutlierDetector.partial_fit]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.partial_fit [hulearn.outlier.functionoutlier.FunctionOutlierDetector.predict]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.predict [hulearn.outlier.functionoutlier.FunctionOutlierDetector.set_params]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.set_params [hulearn.outlier.interactiveoutlier]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.__init__]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.__init__ [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.fit]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.fit [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.from_json]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.from_json [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.poly_data]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.poly_data [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.predict]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.predict [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.score]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.score [hulearn.preprocessing.pipetransformer]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer [hulearn.preprocessing.pipetransformer.PipeTransformer]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer [hulearn.preprocessing.pipetransformer.PipeTransformer.__init__]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.__init__ [hulearn.preprocessing.pipetransformer.PipeTransformer.fit]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.fit [hulearn.preprocessing.pipetransformer.PipeTransformer.get_params]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.get_params [hulearn.preprocessing.pipetransformer.PipeTransformer.partial_fit]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.partial_fit [hulearn.preprocessing.pipetransformer.PipeTransformer.set_params]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.set_params [hulearn.preprocessing.pipetransformer.PipeTransformer.transform]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.transform [hulearn.preprocessing.interactivepreprocessor]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.__init__]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.__init__ [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.fit]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.fit [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.from_json]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.from_json [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.pandas_pipe]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.pandas_pipe [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.poly_data]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.poly_data [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.transform]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.transform [hulearn.experimental.interactive.InteractiveCharts]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts [hulearn.experimental.interactive.InteractiveCharts.__init__]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.__init__ [hulearn.experimental.interactive.InteractiveCharts.add_chart]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.add_chart [hulearn.experimental.interactive.InteractiveCharts.data]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.data [hulearn.experimental.interactive.InteractiveCharts.to_json]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.to_json [hulearn.experimental.interactive.parallel_coordinates]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.parallel_coordinates [hulearn.common]: /human-learn/api/common.html#hulearn.common [hulearn.common.df_to_dictlist]: /human-learn/api/common.html#hulearn.common.df_to_dictlist [hulearn.common.flatten]: /human-learn/api/common.html#hulearn.common.flatten [hulearn.datasets]: /human-learn/api/datasets.html#hulearn.datasets [hulearn.datasets.load_fish]: /human-learn/api/datasets.html#hulearn.datasets.load_fish [hulearn.datasets.load_titanic]: /human-learn/api/datasets.html#hulearn.datasets.load_titanic [hulearn.experimental.ruler.CaseWhenRuler]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler [hulearn.experimental.ruler.CaseWhenRuler.__init__]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.__init__ [hulearn.experimental.ruler.CaseWhenRuler.add_rule]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.add_rule [hulearn.experimental.ruler.CaseWhenRuler.predict]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.predict [hulearn.experimental.ruler.CaseWhenRuler.transform]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.transform","title":"Model Mining"},{"location":"examples/model-mining.html#the-task","text":"We're going to make a rule based model for the creditcard dataset. The main feature of the dataset is that it is suffering from a class imbalance. Instead of training a machine learning model, let's try to instead explore it with a parallel coordinates chart. If you scroll all the way to the bottom of this tutorial you'll see an example of such a chart. It shows a \"train\"-set. We explored the data just like in the video and that led us to define the following model. from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , age = 15 ): ruler = CaseWhenRuler ( default = 0 ) ( ruler . add_rule ( lambda d : ( d [ 'V11' ] > 4 ), 1 ) . add_rule ( lambda d : ( d [ 'V17' ] < - 3 ), 1 ) . add_rule ( lambda d : ( d [ 'V14' ] < - 8 ), 1 )) return ruler . predict ( dataf ) clf = FunctionClassifier ( make_prediction ) Full Code First we load the data. from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split df_credit = fetch_openml ( data_id = 1597 , as_frame = True ) credit_train , credit_test = train_test_split ( df_credit , test_size = 0.5 , shuffle = True ) Next, we create a hiplot in jupyter. import json import hiplot as hip samples = [ credit_train . loc [ lambda d : d [ 'group' ] == True ], credit_train . sample ( 5000 )] json_data = pd . concat ( samples ) . to_json ( orient = 'records' ) hip . Experiment . from_iterable ( json . loads ( json_data )) . display () Given that we have our model, we can make a classification report. from sklearn.metrics import classification_report # Note that `fit` is a no-op here. preds = clf . fit ( credit_train , credit_train [ 'group' ]) . predict ( credit_test )) print ( classification_report ( credit_test [ 'group' ], preds ) When we ran the benchmark locally, we got the following classification report. precision recall f1-score support False 1.00 1.00 1.00 142165 True 0.70 0.73 0.71 239 accuracy 1.00 142404 macro avg 0.85 0.86 0.86 142404 weighted avg 1.00 1.00 1.00 142404","title":"The Task"},{"location":"examples/model-mining.html#deep-learning","text":"It's not a perfect benchmark, but we could compare this result to the one that's demonstrated on the keras blog . The trained model there lists 86.67% precision but only 23.9% recall. Depending on your preferences for false-positives, you could argue that our model is outperforming the deep learning model. It's not 100% a fair comparison. You can imagine that the keras blogpost is written to explain keras. The auther likely didn't attempt to make a state-of-the-art model. But what this demo does show is the merit of turning an exploratory data analysis into a model. You can end up with a very interpretable model, you might learn something about your data along the way and the model might certainly still perform well.","title":"Deep Learning"},{"location":"examples/model-mining.html#parallel-coordinates","text":"If you hover of the group name and right-click, you'll be able to set it for coloring and repeat the experience in the video. By doing that it becomes quite easy to eyeball how to separate the two classes. The V17 column especially seems powerful here. In real life we might ask \"why?\" this column is so distinctive but for now we'll just play around until we find a sensible model. [hulearn.classification.functionclassifier]: /human-learn/api/classification.html#hulearn.classification.functionclassifier [hulearn.classification.functionclassifier.FunctionClassifier]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier [hulearn.classification.functionclassifier.FunctionClassifier.__init__]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.__init__ [hulearn.classification.functionclassifier.FunctionClassifier.fit]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.fit [hulearn.classification.functionclassifier.FunctionClassifier.get_params]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.get_params [hulearn.classification.functionclassifier.FunctionClassifier.partial_fit]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.partial_fit [hulearn.classification.functionclassifier.FunctionClassifier.predict]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.predict [hulearn.classification.functionclassifier.FunctionClassifier.set_params]: /human-learn/api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.set_params [hulearn.classification.interactiveclassifier]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier [hulearn.classification.interactiveclassifier.InteractiveClassifier]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier [hulearn.classification.interactiveclassifier.InteractiveClassifier.__init__]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.__init__ [hulearn.classification.interactiveclassifier.InteractiveClassifier.fit]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.fit [hulearn.classification.interactiveclassifier.InteractiveClassifier.from_json]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.from_json [hulearn.classification.interactiveclassifier.InteractiveClassifier.poly_data]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.poly_data [hulearn.classification.interactiveclassifier.InteractiveClassifier.predict]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.predict [hulearn.classification.interactiveclassifier.InteractiveClassifier.predict_proba]: /human-learn/api/classification.html#hulearn.classification.interactiveclassifier.InteractiveClassifier.predict_proba [hulearn.regression.functionregressor]: /human-learn/api/regression.html#hulearn.regression.functionregressor [hulearn.regression.functionregressor.FunctionRegressor]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor [hulearn.regression.functionregressor.FunctionRegressor.__init__]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.__init__ [hulearn.regression.functionregressor.FunctionRegressor.fit]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.fit [hulearn.regression.functionregressor.FunctionRegressor.get_params]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.get_params [hulearn.regression.functionregressor.FunctionRegressor.partial_fit]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.partial_fit [hulearn.regression.functionregressor.FunctionRegressor.predict]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.predict [hulearn.regression.functionregressor.FunctionRegressor.set_params]: /human-learn/api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.set_params [hulearn.outlier.functionoutlier]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier [hulearn.outlier.functionoutlier.FunctionOutlierDetector]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector [hulearn.outlier.functionoutlier.FunctionOutlierDetector.__init__]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.__init__ [hulearn.outlier.functionoutlier.FunctionOutlierDetector.fit]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.fit [hulearn.outlier.functionoutlier.FunctionOutlierDetector.get_params]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.get_params [hulearn.outlier.functionoutlier.FunctionOutlierDetector.partial_fit]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.partial_fit [hulearn.outlier.functionoutlier.FunctionOutlierDetector.predict]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.predict [hulearn.outlier.functionoutlier.FunctionOutlierDetector.set_params]: /human-learn/api/outlier.html#hulearn.outlier.functionoutlier.FunctionOutlierDetector.set_params [hulearn.outlier.interactiveoutlier]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.__init__]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.__init__ [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.fit]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.fit [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.from_json]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.from_json [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.poly_data]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.poly_data [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.predict]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.predict [hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.score]: /human-learn/api/outlier.html#hulearn.outlier.interactiveoutlier.InteractiveOutlierDetector.score [hulearn.preprocessing.pipetransformer]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer [hulearn.preprocessing.pipetransformer.PipeTransformer]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer [hulearn.preprocessing.pipetransformer.PipeTransformer.__init__]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.__init__ [hulearn.preprocessing.pipetransformer.PipeTransformer.fit]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.fit [hulearn.preprocessing.pipetransformer.PipeTransformer.get_params]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.get_params [hulearn.preprocessing.pipetransformer.PipeTransformer.partial_fit]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.partial_fit [hulearn.preprocessing.pipetransformer.PipeTransformer.set_params]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.set_params [hulearn.preprocessing.pipetransformer.PipeTransformer.transform]: /human-learn/api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.transform [hulearn.preprocessing.interactivepreprocessor]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.__init__]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.__init__ [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.fit]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.fit [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.from_json]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.from_json [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.pandas_pipe]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.pandas_pipe [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.poly_data]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.poly_data [hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.transform]: /human-learn/api/preprocessing.html#hulearn.preprocessing.interactivepreprocessor.InteractivePreprocessor.transform [hulearn.experimental.interactive.InteractiveCharts]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts [hulearn.experimental.interactive.InteractiveCharts.__init__]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.__init__ [hulearn.experimental.interactive.InteractiveCharts.add_chart]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.add_chart [hulearn.experimental.interactive.InteractiveCharts.data]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.data [hulearn.experimental.interactive.InteractiveCharts.to_json]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.InteractiveCharts.to_json [hulearn.experimental.interactive.parallel_coordinates]: /human-learn/api/interactive-charts.html#hulearn.experimental.interactive.parallel_coordinates [hulearn.common]: /human-learn/api/common.html#hulearn.common [hulearn.common.df_to_dictlist]: /human-learn/api/common.html#hulearn.common.df_to_dictlist [hulearn.common.flatten]: /human-learn/api/common.html#hulearn.common.flatten [hulearn.datasets]: /human-learn/api/datasets.html#hulearn.datasets [hulearn.datasets.load_fish]: /human-learn/api/datasets.html#hulearn.datasets.load_fish [hulearn.datasets.load_titanic]: /human-learn/api/datasets.html#hulearn.datasets.load_titanic [hulearn.experimental.ruler.CaseWhenRuler]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler [hulearn.experimental.ruler.CaseWhenRuler.__init__]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.__init__ [hulearn.experimental.ruler.CaseWhenRuler.add_rule]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.add_rule [hulearn.experimental.ruler.CaseWhenRuler.predict]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.predict [hulearn.experimental.ruler.CaseWhenRuler.transform]: /human-learn/api/rulers.html#hulearn.experimental.ruler.CaseWhenRuler.transform","title":"Parallel Coordinates"},{"location":"examples/usage.html","text":"This page contains a list of short examples that demonstrate the utility of the tools in this package. The goal for each example is to be small and consise. Precision and Subgroups \u00b6 It can be the case that for a subgroup of the population you do not need a model. Suppose that we have a session log dataset from \"World of Warcraft\". We know when people logged in, if they were part of a guild and when they stopped playing. You can create a machine learning model to predict which players are at risk of quitting the game but you might also be able to come up with some simple rules. Here is one rule that might work out swell: \"If any player was playing the video game at 24:00 on new-years eve, odds are that this person is very invested in the game and won't stop playing.\" This one rule will not cover the entire population but for the subgroup it can be an effective rule. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier classifier = SomeScikitLearnModel () def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # Override model prediction if a user is a heavy_user, no matter what res = np . where ( dataf [ 'heavy_user' ], \"stays\" , res ) return res fallback_model = FunctionClassifier ( make_decision ) No Data No Problem \u00b6 Let's say that we're interested in detecting fraud at a tax office. Even without looking at the data we can already come up with some sensible rules. Any minor making over the median income is \"suspicious\". Any person who started more than 2 companies in a year is \"suspicious\". Any person who has more than 10 bank accounts is \"suspicious\". The thing with these rules is that they are easy to explain but they are not based on data at all. In fact, they may not occur in the data at all. This means that a machine learning model may not have picked up this pattern that we're interested in. Thankfully, the lack in data can be compensated with business rules. Comfort Zone \u00b6 Models typically have a \"comfort zone\". If a new data point comes in that is very different from what the models saw before it should not be treated the same way. You can also argue that points with low proba score should also not be automated. If you want to prevent predictions where the model is \"unsure\" then you might want to follow this diagram; You can construct such a system by creating a FunctionClassifier that handles the logic you require. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier # We're importing a classifier/outlier detector from our library # but nothing is stopping you from using those in scikit-learn. # Just make sure that they are trained beforehand! outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision ) For more information on why this tactic is helpful: blogpost pydata talk","title":"Usage"},{"location":"examples/usage.html#precision-and-subgroups","text":"It can be the case that for a subgroup of the population you do not need a model. Suppose that we have a session log dataset from \"World of Warcraft\". We know when people logged in, if they were part of a guild and when they stopped playing. You can create a machine learning model to predict which players are at risk of quitting the game but you might also be able to come up with some simple rules. Here is one rule that might work out swell: \"If any player was playing the video game at 24:00 on new-years eve, odds are that this person is very invested in the game and won't stop playing.\" This one rule will not cover the entire population but for the subgroup it can be an effective rule. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier classifier = SomeScikitLearnModel () def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # Override model prediction if a user is a heavy_user, no matter what res = np . where ( dataf [ 'heavy_user' ], \"stays\" , res ) return res fallback_model = FunctionClassifier ( make_decision )","title":"Precision and Subgroups"},{"location":"examples/usage.html#no-data-no-problem","text":"Let's say that we're interested in detecting fraud at a tax office. Even without looking at the data we can already come up with some sensible rules. Any minor making over the median income is \"suspicious\". Any person who started more than 2 companies in a year is \"suspicious\". Any person who has more than 10 bank accounts is \"suspicious\". The thing with these rules is that they are easy to explain but they are not based on data at all. In fact, they may not occur in the data at all. This means that a machine learning model may not have picked up this pattern that we're interested in. Thankfully, the lack in data can be compensated with business rules.","title":"No Data No Problem"},{"location":"examples/usage.html#comfort-zone","text":"Models typically have a \"comfort zone\". If a new data point comes in that is very different from what the models saw before it should not be treated the same way. You can also argue that points with low proba score should also not be automated. If you want to prevent predictions where the model is \"unsure\" then you might want to follow this diagram; You can construct such a system by creating a FunctionClassifier that handles the logic you require. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier # We're importing a classifier/outlier detector from our library # but nothing is stopping you from using those in scikit-learn. # Just make sure that they are trained beforehand! outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision ) For more information on why this tactic is helpful: blogpost pydata talk","title":"Comfort Zone"},{"location":"guide/drawing-classifier/drawing.html","text":"Drawing as a Model \u00b6 Classic Classification Problem \u00b6 Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier. Let's Draw! \u00b6 Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas. Instructions \u00b6 To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model? Properties of Modelling Technique \u00b6 Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model. From Jupyter \u00b6 In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this; Serialize \u00b6 You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made. Model \u00b6 This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API. Conclusion \u00b6 The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems. Notebook \u00b6 If you want to run this code yourself, feel free to download the notebook .","title":"Drawing as a Model"},{"location":"guide/drawing-classifier/drawing.html#drawing-as-a-model","text":"","title":"Drawing as a Model"},{"location":"guide/drawing-classifier/drawing.html#classic-classification-problem","text":"Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier.","title":"Classic Classification Problem"},{"location":"guide/drawing-classifier/drawing.html#lets-draw","text":"Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas.","title":"Let's Draw!"},{"location":"guide/drawing-classifier/drawing.html#instructions","text":"To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model?","title":"Instructions"},{"location":"guide/drawing-classifier/drawing.html#properties-of-modelling-technique","text":"Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model.","title":"Properties of Modelling Technique"},{"location":"guide/drawing-classifier/drawing.html#from-jupyter","text":"In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this;","title":"From Jupyter"},{"location":"guide/drawing-classifier/drawing.html#serialize","text":"You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made.","title":"Serialize"},{"location":"guide/drawing-classifier/drawing.html#model","text":"This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API.","title":"Model"},{"location":"guide/drawing-classifier/drawing.html#conclusion","text":"The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems.","title":"Conclusion"},{"location":"guide/drawing-classifier/drawing.html#notebook","text":"If you want to run this code yourself, feel free to download the notebook .","title":"Notebook"},{"location":"guide/drawing-features/custom-features.html","text":"Sofar we've explored drawing as a tool for models, but it can also be used as a tool to generate features. To explore this, let's load in the penguins dataset again. from sklego.datasets import load_penguins df = load_penguins ( as_frame = True ) . dropna () Drawing \u00b6 We can draw over this dataset. It's like before but with one crucial differenc from hulearn.experimental.interactive import InteractiveCharts # Note that the `labels` arugment here is a list, not a string! This # tells the tool that we want to be able to add custom groups that are # not defined by a column in the dataframe. charts = InteractiveCharts ( df , labels = [ 'group_one' , 'group_two' ]) Let's make a custom drawing. charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) Let's assume the new drawing looks something like this. Sofar these drawn features have been used to construct models. But they can also be used to help label data or generate extra features for machine learning models. Features \u00b6 This library makes it easy to add these features to scikit-learn pipelines or to pandas. To get started, you'll want to import the InteractivePreprocessor . from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor ( json_desc = charts . data ()) This tfm object is can be used as a preprocessing step inside of scikit-learn but it can also be used in a pandas pipeline. # The flow for scikit-learn tfm . fit ( df ) . transform ( df ) # The flow for pandas df . pipe ( tfm . pandas_pipe )","title":"Drawing Features"},{"location":"guide/drawing-features/custom-features.html#drawing","text":"We can draw over this dataset. It's like before but with one crucial differenc from hulearn.experimental.interactive import InteractiveCharts # Note that the `labels` arugment here is a list, not a string! This # tells the tool that we want to be able to add custom groups that are # not defined by a column in the dataframe. charts = InteractiveCharts ( df , labels = [ 'group_one' , 'group_two' ]) Let's make a custom drawing. charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) Let's assume the new drawing looks something like this. Sofar these drawn features have been used to construct models. But they can also be used to help label data or generate extra features for machine learning models.","title":"Drawing"},{"location":"guide/drawing-features/custom-features.html#features","text":"This library makes it easy to add these features to scikit-learn pipelines or to pandas. To get started, you'll want to import the InteractivePreprocessor . from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor ( json_desc = charts . data ()) This tfm object is can be used as a preprocessing step inside of scikit-learn but it can also be used in a pandas pipeline. # The flow for scikit-learn tfm . fit ( df ) . transform ( df ) # The flow for pandas df . pipe ( tfm . pandas_pipe )","title":"Features"},{"location":"guide/finding-outliers/outliers.html","text":"Rethinking Classification \u00b6 Let's have another look at the interactive canvas that we saw in the previous guide. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The drawn colors indicate that a human deemed a classification appropriate. You could wonder what we might want to do with the regions that have not been colored though. Machine learning algorithms might typically still assign a class to those regions but that can be a dangerous idea. We might consider these points outside of the \"comfort zone\" of the predicted areas. In these situations it might be best to declare it an outlier and to handle it differently. That way we don't automate a decision that we're likely to regret later. Outliers \u00b6 The drawn charts can be used to construct a classifier but they may also be used to construct an outlier detection model. This allows us to re-use earlier work for multiple purposes. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () charts = InteractiveCharts ( df , labels = \"species\" ) # Run this in a seperate cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Run this in a seperate cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) To demonstrate how it works, let's assume that we've drawn the following: We'll again fetch the drawn data but now we'll use it to detect outliers. from hulearn.outlier import InteractiveOutlierDetector # Load the model using drawn-data. model = InteractiveOutlierDetector ( json_desc = charts . data ()) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict ( X ) This model can now be used as a scikit-learn compatible outlier detection model. Here's the output of the model. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 10 , 4 )) plt . subplot ( 121 ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds ) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . subplot ( 122 ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds ) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ); How it works. \u00b6 A point is considered an outlier if it does not fall inside of enough drawn polygons. The number of poylgons that a point must fall into is a parameter that you can set manually or even search for in a grid-search. For example, let's repeat the exercise. The base setting is that a point needs to be in at least one polygon but we can change this to two. # Before model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 1 ) # After model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 2 ) Combine \u00b6 You might wonder, can we combine the FunctionClassifier with an outlier model like we've got here? Yes! Use a FunctionClassifier ! As an illustrative example we'll implement a diagram like above as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision )","title":"Outliers and Comfort"},{"location":"guide/finding-outliers/outliers.html#rethinking-classification","text":"Let's have another look at the interactive canvas that we saw in the previous guide. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The drawn colors indicate that a human deemed a classification appropriate. You could wonder what we might want to do with the regions that have not been colored though. Machine learning algorithms might typically still assign a class to those regions but that can be a dangerous idea. We might consider these points outside of the \"comfort zone\" of the predicted areas. In these situations it might be best to declare it an outlier and to handle it differently. That way we don't automate a decision that we're likely to regret later.","title":"Rethinking Classification"},{"location":"guide/finding-outliers/outliers.html#outliers","text":"The drawn charts can be used to construct a classifier but they may also be used to construct an outlier detection model. This allows us to re-use earlier work for multiple purposes. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () charts = InteractiveCharts ( df , labels = \"species\" ) # Run this in a seperate cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Run this in a seperate cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) To demonstrate how it works, let's assume that we've drawn the following: We'll again fetch the drawn data but now we'll use it to detect outliers. from hulearn.outlier import InteractiveOutlierDetector # Load the model using drawn-data. model = InteractiveOutlierDetector ( json_desc = charts . data ()) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict ( X ) This model can now be used as a scikit-learn compatible outlier detection model. Here's the output of the model. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 10 , 4 )) plt . subplot ( 121 ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds ) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . subplot ( 122 ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds ) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' );","title":"Outliers"},{"location":"guide/finding-outliers/outliers.html#how-it-works","text":"A point is considered an outlier if it does not fall inside of enough drawn polygons. The number of poylgons that a point must fall into is a parameter that you can set manually or even search for in a grid-search. For example, let's repeat the exercise. The base setting is that a point needs to be in at least one polygon but we can change this to two. # Before model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 1 ) # After model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 2 )","title":"How it works."},{"location":"guide/finding-outliers/outliers.html#combine","text":"You might wonder, can we combine the FunctionClassifier with an outlier model like we've got here? Yes! Use a FunctionClassifier ! As an illustrative example we'll implement a diagram like above as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision )","title":"Combine"},{"location":"guide/function-classifier/function-classifier.html","text":"The goal of this library is to make it easier to declare common sense models. A very pythonic way of getting there is to declare a function. One of the first features in this library is the ability to re-use functions as if they are scikit-learn models. Titanic \u00b6 Let's see how this might work. We'll grab a dataset that is packaged along with this library. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) df . head () The df variable represents a dataframe and it has the following contents: survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe. Preparation \u00b6 To prepare our data we will first get it into the common X , y format for scikit-learn. X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense? It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better. FunctionClassifier \u00b6 Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the FunctionClassifier . You can see an example of that below. import numpy as np from hulearn.classification import FunctionClassifier def fare_based ( dataf , threshold = 10 ): \"\"\" The assumption is that folks who paid more are wealthier and are more likely to have recieved access to lifeboats. \"\"\" return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) mod = FunctionClassifier ( fare_based ) This mod is a scikit-learn model, which means that you can .fit(X, y).predict(X) . mod . fit ( X , y ) . predict ( X ) During the .fit(X, y) -step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step. Grid \u00b6 Being able to .fit(X, y).predict(X) is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best threshold value? For that, you might like to use GridSearchCV . from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score , recall_score , accuracy_score , make_scorer # Note the threshold keyword argument in this function. def fare_based ( dataf , threshold = 10 ): return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) # Pay attention here, we set the threshold argument in here. mod = FunctionClassifier ( fare_based , threshold = 10 ) # The GridSearch object can now \"grid-search\" over this argument. # We also add a bunch of metrics to our approach so we can measure. grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'threshold' : np . linspace ( 0 , 100 , 30 )}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) If we make a chart of the grid.cv_results_ then they would look something like; A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a RandomForestClassifier using the 'pclass', 'sex', 'age', 'fare' columns that the precision score would be about the same. Bigger Grids \u00b6 You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that. def last_name ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) # Once again, remember to declare your arguments here too! mod = FunctionClassifier ( last_name , pclass = 10 , sex = 'male' ) # The arguments of the function can now be \"grid-searched\". grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) Guidance \u00b6 Human Learn doesn't just allow you to turn functions into classifiers. It also tries to help you find rules that could be useful. In particular, an interactive parallel coordinates chart could be very helpful here. You can create a parallel coordinates chart directly inside of jupyter. from hulearn.experimental.interactive import parallel_coordinates parallel_coordinates ( df , label = \"survived\" , height = 200 ) What follows next are some explorations of the dataset. They are based on the scene from the titanic movie where they yell \"Woman and Children First!\". So let's see if we can confirm if this holds true. Explore \u00b6 It indeed seems that women in 1st/2nd class have a high chance of surviving. It also seems that male children have an increased change of survival, but only if they were travelling 1st/2nd class. Grid \u00b6 Here's a lovely observation. By doing exploratory analysis we not only understand the data better but we can now also turn the patterns that we've observed into a model! def make_prediction ( dataf , age = 15 ): women_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'sex' ] == \"female\" ) children_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'age' ] <= age ) return women_rule | children_rule mod = FunctionClassifier ( make_prediction ) We're even able to use grid-search again to find the optimal threshold for \"age\" . Comparison \u00b6 To compare our results we've also trained a RandomForestClassifier . Here's how the models compare; Model accuracy precision recall Women & Children Rule 0.808157 0.952168 0.558621 RandomForestClassifier 0.813869 0.785059 0.751724 It seems like our rule based model is quite reasonable. A great follow-up exercise would be to try and understand when the random forest model disagrees with the rule based system. This could lead us to understand more patterns in the data. Conclusion \u00b6 In this guide we've seen the FunctionClassifier in action. It is one of the many models in this library that will help you construct more \"human\" models. This component is very effective when it is combined with exploratory data analysis techniques. Notebook \u00b6 If you want to download with this code yourself, feel free to download the notebook here .","title":"Function as a Model"},{"location":"guide/function-classifier/function-classifier.html#titanic","text":"Let's see how this might work. We'll grab a dataset that is packaged along with this library. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) df . head () The df variable represents a dataframe and it has the following contents: survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe.","title":"Titanic"},{"location":"guide/function-classifier/function-classifier.html#preparation","text":"To prepare our data we will first get it into the common X , y format for scikit-learn. X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense? It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better.","title":"Preparation"},{"location":"guide/function-classifier/function-classifier.html#functionclassifier","text":"Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the FunctionClassifier . You can see an example of that below. import numpy as np from hulearn.classification import FunctionClassifier def fare_based ( dataf , threshold = 10 ): \"\"\" The assumption is that folks who paid more are wealthier and are more likely to have recieved access to lifeboats. \"\"\" return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) mod = FunctionClassifier ( fare_based ) This mod is a scikit-learn model, which means that you can .fit(X, y).predict(X) . mod . fit ( X , y ) . predict ( X ) During the .fit(X, y) -step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step.","title":"FunctionClassifier"},{"location":"guide/function-classifier/function-classifier.html#grid","text":"Being able to .fit(X, y).predict(X) is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best threshold value? For that, you might like to use GridSearchCV . from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score , recall_score , accuracy_score , make_scorer # Note the threshold keyword argument in this function. def fare_based ( dataf , threshold = 10 ): return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) # Pay attention here, we set the threshold argument in here. mod = FunctionClassifier ( fare_based , threshold = 10 ) # The GridSearch object can now \"grid-search\" over this argument. # We also add a bunch of metrics to our approach so we can measure. grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'threshold' : np . linspace ( 0 , 100 , 30 )}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) If we make a chart of the grid.cv_results_ then they would look something like; A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a RandomForestClassifier using the 'pclass', 'sex', 'age', 'fare' columns that the precision score would be about the same.","title":"Grid"},{"location":"guide/function-classifier/function-classifier.html#bigger-grids","text":"You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that. def last_name ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) # Once again, remember to declare your arguments here too! mod = FunctionClassifier ( last_name , pclass = 10 , sex = 'male' ) # The arguments of the function can now be \"grid-searched\". grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y )","title":"Bigger Grids"},{"location":"guide/function-classifier/function-classifier.html#guidance","text":"Human Learn doesn't just allow you to turn functions into classifiers. It also tries to help you find rules that could be useful. In particular, an interactive parallel coordinates chart could be very helpful here. You can create a parallel coordinates chart directly inside of jupyter. from hulearn.experimental.interactive import parallel_coordinates parallel_coordinates ( df , label = \"survived\" , height = 200 ) What follows next are some explorations of the dataset. They are based on the scene from the titanic movie where they yell \"Woman and Children First!\". So let's see if we can confirm if this holds true.","title":"Guidance"},{"location":"guide/function-classifier/function-classifier.html#explore","text":"It indeed seems that women in 1st/2nd class have a high chance of surviving. It also seems that male children have an increased change of survival, but only if they were travelling 1st/2nd class.","title":"Explore"},{"location":"guide/function-classifier/function-classifier.html#grid_1","text":"Here's a lovely observation. By doing exploratory analysis we not only understand the data better but we can now also turn the patterns that we've observed into a model! def make_prediction ( dataf , age = 15 ): women_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'sex' ] == \"female\" ) children_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'age' ] <= age ) return women_rule | children_rule mod = FunctionClassifier ( make_prediction ) We're even able to use grid-search again to find the optimal threshold for \"age\" .","title":"Grid"},{"location":"guide/function-classifier/function-classifier.html#comparison","text":"To compare our results we've also trained a RandomForestClassifier . Here's how the models compare; Model accuracy precision recall Women & Children Rule 0.808157 0.952168 0.558621 RandomForestClassifier 0.813869 0.785059 0.751724 It seems like our rule based model is quite reasonable. A great follow-up exercise would be to try and understand when the random forest model disagrees with the rule based system. This could lead us to understand more patterns in the data.","title":"Comparison"},{"location":"guide/function-classifier/function-classifier.html#conclusion","text":"In this guide we've seen the FunctionClassifier in action. It is one of the many models in this library that will help you construct more \"human\" models. This component is very effective when it is combined with exploratory data analysis techniques.","title":"Conclusion"},{"location":"guide/function-classifier/function-classifier.html#notebook","text":"If you want to download with this code yourself, feel free to download the notebook here .","title":"Notebook"},{"location":"guide/function-preprocess/function-preprocessing.html","text":"In python the most popular data analysis tool is pandas while the most popular tool for making models is scikit-learn. We love the data wrangling tools of pandas while we appreciate the benchmarking capability of scikit-learn. The fact that these tools don't fully interact is slightly awkward. The data going into the model has an big effect on the output. So how might we more easily combine the two? Pipe \u00b6 In pandas there's an amazing trick that you can do with the .pipe method. We'll give a quick overview on how it works but if you're new to this idea you may appreciate this resource or this blogpost . from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] X . head ( 4 ) The goal of the titanic dataset is to predict weather or not a passenger survived the disaster. The X variable represents a dataframe with variables that we're going to use to predict survival (stored in y ). Here's a preview of what X might have. pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code. X [ 'nchar' ] = X [ 'name' ] . str . len () This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function. def process ( dataf ): # Make a copy of the dataframe to prevent it from overwriting the original data. dataf = dataf . copy () # Make the changes dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () # Return the name dataframe return dataf We now have a nice function that makes our changes and we can use it like so; X_new = process ( X ) We can do something more powerful though. Paramaters \u00b6 Let's make some more changes to our process function. def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end. We've changed the way we've defined our function but we're also changing the way that we're going to apply it. # This is equivalent to preprocessing(X) X . pipe ( preprocessing ) The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview. With .pipe() \u00b6 ( df . pipe ( set_col_types ) . pipe ( preprocessing , nchar = True , gender = False ) . pipe ( add_time_info )) Without .pipe() \u00b6 add_time_info ( preprocessing ( set_col_types ( df ), nchar = True , gender = False )) Let's be honest, this looks messy. PipeTransformer \u00b6 It would be great if we could use the preprocessing -function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas .pipe -line in general! For that we've got another feature in our library, the PipeTransformer . from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) # Important, don't forget to declare `n_char` and `gender` here. tfm = PipeTransformer ( preprocessing , n_char = True , gender = True )) The tfm variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function. from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV pipe = Pipeline ([ ( 'prep' , tfm ), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) Once trained we can fetch the grid.cv_results_ to get a glimpse at the results of our pipeline. param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 It seems that we gender of the passenger has more of an effect on their survival than the length of their name. Utility \u00b6 The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code. There's a few small caveats to be aware of. Don't remove data \u00b6 Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this. Don't sort data \u00b6 You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the y variable that you're trying to predict. Don't use lambda \u00b6 There's two ways that you can add a new column to pandas. # Method 1 dataf_new = dataf . copy () # Don't overwrite data! dataf_new [ 'new_column' ] = dataf_new [ 'old_column' ] * 2 # Method 2 dataf_new = dataf . assign ( lambda d : d [ 'old_column' ] * 2 ) In many cases you might argue that method #2 is safer because you do not need to worry about the dataf.copy() that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use lambda functions because it cannot pickle the code. Don't Cheat! \u00b6 The functions that you write are supposed to be stateless in the sense that they don't learn from the data that goes in. You could theoretically bypass this with global variables but by doing so you're doing yourself a disservice. If you do this you'll be cheating the statistics by leaking information.","title":"Human Preprocessing"},{"location":"guide/function-preprocess/function-preprocessing.html#pipe","text":"In pandas there's an amazing trick that you can do with the .pipe method. We'll give a quick overview on how it works but if you're new to this idea you may appreciate this resource or this blogpost . from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] X . head ( 4 ) The goal of the titanic dataset is to predict weather or not a passenger survived the disaster. The X variable represents a dataframe with variables that we're going to use to predict survival (stored in y ). Here's a preview of what X might have. pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code. X [ 'nchar' ] = X [ 'name' ] . str . len () This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function. def process ( dataf ): # Make a copy of the dataframe to prevent it from overwriting the original data. dataf = dataf . copy () # Make the changes dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () # Return the name dataframe return dataf We now have a nice function that makes our changes and we can use it like so; X_new = process ( X ) We can do something more powerful though.","title":"Pipe"},{"location":"guide/function-preprocess/function-preprocessing.html#paramaters","text":"Let's make some more changes to our process function. def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end. We've changed the way we've defined our function but we're also changing the way that we're going to apply it. # This is equivalent to preprocessing(X) X . pipe ( preprocessing ) The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview.","title":"Paramaters"},{"location":"guide/function-preprocess/function-preprocessing.html#with-pipe","text":"( df . pipe ( set_col_types ) . pipe ( preprocessing , nchar = True , gender = False ) . pipe ( add_time_info ))","title":"With .pipe()"},{"location":"guide/function-preprocess/function-preprocessing.html#without-pipe","text":"add_time_info ( preprocessing ( set_col_types ( df ), nchar = True , gender = False )) Let's be honest, this looks messy.","title":"Without .pipe()"},{"location":"guide/function-preprocess/function-preprocessing.html#pipetransformer","text":"It would be great if we could use the preprocessing -function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas .pipe -line in general! For that we've got another feature in our library, the PipeTransformer . from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) # Important, don't forget to declare `n_char` and `gender` here. tfm = PipeTransformer ( preprocessing , n_char = True , gender = True )) The tfm variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function. from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV pipe = Pipeline ([ ( 'prep' , tfm ), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) Once trained we can fetch the grid.cv_results_ to get a glimpse at the results of our pipeline. param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 It seems that we gender of the passenger has more of an effect on their survival than the length of their name.","title":"PipeTransformer"},{"location":"guide/function-preprocess/function-preprocessing.html#utility","text":"The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code. There's a few small caveats to be aware of.","title":"Utility"},{"location":"guide/function-preprocess/function-preprocessing.html#dont-remove-data","text":"Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this.","title":"Don't remove data"},{"location":"guide/function-preprocess/function-preprocessing.html#dont-sort-data","text":"You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the y variable that you're trying to predict.","title":"Don't sort data"},{"location":"guide/function-preprocess/function-preprocessing.html#dont-use-lambda","text":"There's two ways that you can add a new column to pandas. # Method 1 dataf_new = dataf . copy () # Don't overwrite data! dataf_new [ 'new_column' ] = dataf_new [ 'old_column' ] * 2 # Method 2 dataf_new = dataf . assign ( lambda d : d [ 'old_column' ] * 2 ) In many cases you might argue that method #2 is safer because you do not need to worry about the dataf.copy() that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use lambda functions because it cannot pickle the code.","title":"Don't use lambda"},{"location":"guide/function-preprocess/function-preprocessing.html#dont-cheat","text":"The functions that you write are supposed to be stateless in the sense that they don't learn from the data that goes in. You could theoretically bypass this with global variables but by doing so you're doing yourself a disservice. If you do this you'll be cheating the statistics by leaking information.","title":"Don't Cheat!"}]}