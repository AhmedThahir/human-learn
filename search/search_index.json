{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Human Learn \u00b6 Machine Learning models should play by the rules, literally. This package contains scikit-learn compatible tools that should make it easier to construct and benchmark rule based systems that are designed by humans. Install \u00b6 You can install this tool via pip . python - m pip install human - learn Features \u00b6 This library hosts a couple of models that you can play with. Classification Models \u00b6 FunctionClassifier \u00b6 This allows you to define a function that can make classification predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. Regression Models \u00b6 FunctionRegressor \u00b6 This allows you to define a function that can make regression predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. Preprocessing Models \u00b6 PipeTransformer \u00b6 This allows you to define a function that can make handle preprocessing. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. This is especially powerful in combination with the pandas .pipe method. If you're unfamiliar with this amazing feature, yo may appreciate this tutorial . Datasets \u00b6 load_titanic \u00b6 Loads in the popular titanic survivor dataset. The goal of this dataset is to predict who might have survived the titanic disaster.","title":"Index"},{"location":"index.html#human-learn","text":"Machine Learning models should play by the rules, literally. This package contains scikit-learn compatible tools that should make it easier to construct and benchmark rule based systems that are designed by humans.","title":"Human Learn"},{"location":"index.html#install","text":"You can install this tool via pip . python - m pip install human - learn","title":"Install"},{"location":"index.html#features","text":"This library hosts a couple of models that you can play with.","title":"Features"},{"location":"index.html#classification-models","text":"","title":"Classification Models"},{"location":"index.html#functionclassifier","text":"This allows you to define a function that can make classification predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.","title":"FunctionClassifier"},{"location":"index.html#regression-models","text":"","title":"Regression Models"},{"location":"index.html#functionregressor","text":"This allows you to define a function that can make regression predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.","title":"FunctionRegressor"},{"location":"index.html#preprocessing-models","text":"","title":"Preprocessing Models"},{"location":"index.html#pipetransformer","text":"This allows you to define a function that can make handle preprocessing. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. This is especially powerful in combination with the pandas .pipe method. If you're unfamiliar with this amazing feature, yo may appreciate this tutorial .","title":"PipeTransformer"},{"location":"index.html#datasets","text":"","title":"Datasets"},{"location":"index.html#load_titanic","text":"Loads in the popular titanic survivor dataset. The goal of this dataset is to predict who might have survived the titanic disaster.","title":"load_titanic"},{"location":"api/classification.html","text":"from hulearn.classification import * \u00b6 FunctionClassifier \u00b6 This class allows you to pass a function to make the predictions you're interested in. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! Usage: import numpy as np import pandas as pd from sklearn.model_selection import GridSearchCV from hulearn.datasets import load_titanic from hulearn.classification import FunctionClassifier df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] def class_based ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) mod = FunctionClassifier ( class_based , pclass = 10 ) params = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]} grid = GridSearchCV ( mod , cv = 3 , param_grid = params ) . fit ( X , y ) pd . DataFrame ( grid . cv_results_ ) fit ( self , X , y ) \u00b6 Show source code in classification/functionclassifier.py 44 45 46 47 48 49 50 51 52 53 def fit ( self , X , y ): \"\"\" Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. \"\"\" # Run it to confirm no error happened. self . classes_ = unique_labels ( y ) _ = self . func ( X , ** self . kwargs ) return self Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. predict ( self , X ) \u00b6 Show source code in classification/functionclassifier.py 55 56 57 58 59 60 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"classes_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"Classification"},{"location":"api/classification.html#from-hulearnclassification-import","text":"","title":"from hulearn.classification import *"},{"location":"api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier","text":"This class allows you to pass a function to make the predictions you're interested in. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! Usage: import numpy as np import pandas as pd from sklearn.model_selection import GridSearchCV from hulearn.datasets import load_titanic from hulearn.classification import FunctionClassifier df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] def class_based ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) mod = FunctionClassifier ( class_based , pclass = 10 ) params = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]} grid = GridSearchCV ( mod , cv = 3 , param_grid = params ) . fit ( X , y ) pd . DataFrame ( grid . cv_results_ )","title":"FunctionClassifier"},{"location":"api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.fit","text":"Show source code in classification/functionclassifier.py 44 45 46 47 48 49 50 51 52 53 def fit ( self , X , y ): \"\"\" Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. \"\"\" # Run it to confirm no error happened. self . classes_ = unique_labels ( y ) _ = self . func ( X , ** self . kwargs ) return self Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set.","title":"fit()"},{"location":"api/classification.html#hulearn.classification.functionclassifier.FunctionClassifier.predict","text":"Show source code in classification/functionclassifier.py 55 56 57 58 59 60 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"classes_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"predict()"},{"location":"api/common.html","text":"from hulearn.common import * \u00b6 df_to_dictlist ( dataf ) \u00b6 Show source code in hulearn/common.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def df_to_dictlist ( dataf ): \"\"\" Helper function, takes a dataframe and turns it into a list of dictionaries. This might make it easier to write if else chains in `FunctionClassifier`. Usage: ```python import pandas as pd from hulearn.common import df_to_dictlist df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) res = df_to_dictlist(df) assert res == [{\"a\": 1, \"b\": 4}, {\"a\": 2, \"b\": 5}, {\"a\": 3, \"b\": 6}] ``` \"\"\" data = dataf . iterrows () return [ dict ( d ) for i , d in data ] Helper function, takes a dataframe and turns it into a list of dictionaries. This might make it easier to write if else chains in FunctionClassifier . Usage: import pandas as pd from hulearn.common import df_to_dictlist df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) res = df_to_dictlist ( df ) assert res == [{ \"a\" : 1 , \"b\" : 4 }, { \"a\" : 2 , \"b\" : 5 }, { \"a\" : 3 , \"b\" : 6 }] flatten ( nested_iterable ) \u00b6 Show source code in hulearn/common.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def flatten ( nested_iterable ): \"\"\" Helper function, returns an iterator of flattened values from an arbitrarily nested iterable. Usage: ```python from hulearn.common import flatten res1 = list(flatten([['test1', 'test2'], ['a', 'b', ['c', 'd']]])) res2 = list(flatten(['test1', ['test2']])) assert res1 == ['test1', 'test2', 'a', 'b', 'c', 'd'] assert res2 == ['test1', 'test2'] ``` \"\"\" for el in nested_iterable : if isinstance ( el , collections . abc . Iterable ) and not isinstance ( el , ( str , bytes ) ): yield from flatten ( el ) else : yield el Helper function, returns an iterator of flattened values from an arbitrarily nested iterable. Usage: from hulearn.common import flatten res1 = list ( flatten ([[ 'test1' , 'test2' ], [ 'a' , 'b' , [ 'c' , 'd' ]]])) res2 = list ( flatten ([ 'test1' , [ 'test2' ]])) assert res1 == [ 'test1' , 'test2' , 'a' , 'b' , 'c' , 'd' ] assert res2 == [ 'test1' , 'test2' ]","title":"Common"},{"location":"api/common.html#from-hulearncommon-import","text":"","title":"from hulearn.common import *"},{"location":"api/common.html#hulearn.common.df_to_dictlist","text":"Show source code in hulearn/common.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def df_to_dictlist ( dataf ): \"\"\" Helper function, takes a dataframe and turns it into a list of dictionaries. This might make it easier to write if else chains in `FunctionClassifier`. Usage: ```python import pandas as pd from hulearn.common import df_to_dictlist df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]}) res = df_to_dictlist(df) assert res == [{\"a\": 1, \"b\": 4}, {\"a\": 2, \"b\": 5}, {\"a\": 3, \"b\": 6}] ``` \"\"\" data = dataf . iterrows () return [ dict ( d ) for i , d in data ] Helper function, takes a dataframe and turns it into a list of dictionaries. This might make it easier to write if else chains in FunctionClassifier . Usage: import pandas as pd from hulearn.common import df_to_dictlist df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 4 , 5 , 6 ]}) res = df_to_dictlist ( df ) assert res == [{ \"a\" : 1 , \"b\" : 4 }, { \"a\" : 2 , \"b\" : 5 }, { \"a\" : 3 , \"b\" : 6 }]","title":"df_to_dictlist()"},{"location":"api/common.html#hulearn.common.flatten","text":"Show source code in hulearn/common.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def flatten ( nested_iterable ): \"\"\" Helper function, returns an iterator of flattened values from an arbitrarily nested iterable. Usage: ```python from hulearn.common import flatten res1 = list(flatten([['test1', 'test2'], ['a', 'b', ['c', 'd']]])) res2 = list(flatten(['test1', ['test2']])) assert res1 == ['test1', 'test2', 'a', 'b', 'c', 'd'] assert res2 == ['test1', 'test2'] ``` \"\"\" for el in nested_iterable : if isinstance ( el , collections . abc . Iterable ) and not isinstance ( el , ( str , bytes ) ): yield from flatten ( el ) else : yield el Helper function, returns an iterator of flattened values from an arbitrarily nested iterable. Usage: from hulearn.common import flatten res1 = list ( flatten ([[ 'test1' , 'test2' ], [ 'a' , 'b' , [ 'c' , 'd' ]]])) res2 = list ( flatten ([ 'test1' , [ 'test2' ]])) assert res1 == [ 'test1' , 'test2' , 'a' , 'b' , 'c' , 'd' ] assert res2 == [ 'test1' , 'test2' ]","title":"flatten()"},{"location":"api/datasets.html","text":"from hulearn.datasets import * \u00b6 load_titanic ( return_X_y = False , as_frame = False ) \u00b6 Show source code in hulearn/datasets.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def load_titanic ( return_X_y : bool = False , as_frame : bool = False ): \"\"\" Loads in a subset of the titanic dataset. You can find the full dataset [here](https://www.kaggle.com/c/titanic/data). Arguments: return_X_y: return a tuple of (`X`, `y`) for convenience as_frame: return all the data as a pandas dataframe Usage: ```python from hulearn.datasets import load_titanic df = load_titanic(as_frame=True) X, y = load_titanic(return_X_y=True) ``` \"\"\" filepath = resource_filename ( \"hulearn\" , os . path . join ( \"data\" , \"titanic.zip\" )) df = pd . read_csv ( filepath ) if as_frame : return df X , y = ( df [[ \"pclass\" , \"name\" , \"sex\" , \"age\" , \"fare\" , \"sibsp\" , \"parch\" ]] . values , df [ \"survived\" ] . values , ) if return_X_y : return X , y return { \"data\" : X , \"target\" : y } Loads in a subset of the titanic dataset. You can find the full dataset here . Parameters Name Type Description Default return_X_y bool return a tuple of ( X , y ) for convenience False as_frame bool return all the data as a pandas dataframe False Usage: from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = load_titanic ( return_X_y = True )","title":"Datasets"},{"location":"api/datasets.html#from-hulearndatasets-import","text":"","title":"from hulearn.datasets import *"},{"location":"api/datasets.html#hulearn.datasets.load_titanic","text":"Show source code in hulearn/datasets.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def load_titanic ( return_X_y : bool = False , as_frame : bool = False ): \"\"\" Loads in a subset of the titanic dataset. You can find the full dataset [here](https://www.kaggle.com/c/titanic/data). Arguments: return_X_y: return a tuple of (`X`, `y`) for convenience as_frame: return all the data as a pandas dataframe Usage: ```python from hulearn.datasets import load_titanic df = load_titanic(as_frame=True) X, y = load_titanic(return_X_y=True) ``` \"\"\" filepath = resource_filename ( \"hulearn\" , os . path . join ( \"data\" , \"titanic.zip\" )) df = pd . read_csv ( filepath ) if as_frame : return df X , y = ( df [[ \"pclass\" , \"name\" , \"sex\" , \"age\" , \"fare\" , \"sibsp\" , \"parch\" ]] . values , df [ \"survived\" ] . values , ) if return_X_y : return X , y return { \"data\" : X , \"target\" : y } Loads in a subset of the titanic dataset. You can find the full dataset here . Parameters Name Type Description Default return_X_y bool return a tuple of ( X , y ) for convenience False as_frame bool return all the data as a pandas dataframe False Usage: from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = load_titanic ( return_X_y = True )","title":"load_titanic()"},{"location":"api/preprocessing.html","text":"from hulearn.preprocessing import * \u00b6 PipeTransformer \u00b6 This transformer allows you to define a function that will take in data and transform it however you like. You can specify keyword arguments that you can benchmark as well. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! Usage: import pandas as pd from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV from hulearn.datasets import load_titanic from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () # I'm not using .assign() in this pipeline because lambda functions # do not pickle and GridSearchCV demands that it can. if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] pipe = Pipeline ([ ( 'prep' , PipeTransformer ( preprocessing , n_char = True , gender = True )), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) pd . DataFrame ( grid . cv_results_ )[[ 'param_prep__gender' , 'param_prep__n_char' , 'mean_test_score' ]] fit ( self , X , y = None ) \u00b6 Show source code in preprocessing/pipetransformer.py 62 63 64 65 66 67 68 69 70 71 72 def fit ( self , X , y = None ): \"\"\" Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True self . ncol_ = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] return self Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. transform ( self , X ) \u00b6 Show source code in preprocessing/pipetransformer.py 74 75 76 77 78 79 80 81 82 83 84 def transform ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" , \"ncol_\" ]) ncol = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] if self . ncol_ != ncol : raise ValueError ( f \"Reshape your data, there were { self . ncol_ } features during training, now= { ncol } .\" ) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"Preprocessing"},{"location":"api/preprocessing.html#from-hulearnpreprocessing-import","text":"","title":"from hulearn.preprocessing import *"},{"location":"api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer","text":"This transformer allows you to define a function that will take in data and transform it however you like. You can specify keyword arguments that you can benchmark as well. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! Usage: import pandas as pd from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV from hulearn.datasets import load_titanic from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () # I'm not using .assign() in this pipeline because lambda functions # do not pickle and GridSearchCV demands that it can. if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] pipe = Pipeline ([ ( 'prep' , PipeTransformer ( preprocessing , n_char = True , gender = True )), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) pd . DataFrame ( grid . cv_results_ )[[ 'param_prep__gender' , 'param_prep__n_char' , 'mean_test_score' ]]","title":"PipeTransformer"},{"location":"api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.fit","text":"Show source code in preprocessing/pipetransformer.py 62 63 64 65 66 67 68 69 70 71 72 def fit ( self , X , y = None ): \"\"\" Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True self . ncol_ = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] return self Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set.","title":"fit()"},{"location":"api/preprocessing.html#hulearn.preprocessing.pipetransformer.PipeTransformer.transform","text":"Show source code in preprocessing/pipetransformer.py 74 75 76 77 78 79 80 81 82 83 84 def transform ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" , \"ncol_\" ]) ncol = 0 if len ( X . shape ) == 1 else X . shape [ 1 ] if self . ncol_ != ncol : raise ValueError ( f \"Reshape your data, there were { self . ncol_ } features during training, now= { ncol } .\" ) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"transform()"},{"location":"api/regression.html","text":"from hulearn.regression import * \u00b6 FunctionRegressor \u00b6 This class allows you to pass a function to make the predictions you're interested in. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions! fit ( self , X , y ) \u00b6 Show source code in regression/functionregressor.py 20 21 22 23 24 25 26 27 28 29 def fit ( self , X , y ): \"\"\" Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True return self Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. predict ( self , X ) \u00b6 Show source code in regression/functionregressor.py 31 32 33 34 35 36 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"Regression"},{"location":"api/regression.html#from-hulearnregression-import","text":"","title":"from hulearn.regression import *"},{"location":"api/regression.html#hulearn.regression.functionregressor.FunctionRegressor","text":"This class allows you to pass a function to make the predictions you're interested in. Parameters Name Type Description Default func the function that can make predictions required **kwargs extra keyword arguments will be pass to the function, can be grid-search-able {} The functions that are passed need to be pickle-able. That means no lambda functions!","title":"FunctionRegressor"},{"location":"api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.fit","text":"Show source code in regression/functionregressor.py 20 21 22 23 24 25 26 27 28 29 def fit ( self , X , y ): \"\"\" Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set. \"\"\" # Run it to confirm no error happened. _ = self . func ( X , ** self . kwargs ) self . fitted_ = True return self Fit the classifier. This classifier tries to confirm if the passed function can predict appropriate values on the train set.","title":"fit()"},{"location":"api/regression.html#hulearn.regression.functionregressor.FunctionRegressor.predict","text":"Show source code in regression/functionregressor.py 31 32 33 34 35 36 def predict ( self , X ): \"\"\" Make predictions using the passed function. \"\"\" check_is_fitted ( self , [ \"fitted_\" ]) return self . func ( X , ** self . kwargs ) Make predictions using the passed function.","title":"predict()"},{"location":"guide/function-classifier.html","text":"The goal of this library is to make it easier to declare common sense models. A very pythonic way of getting there is to declare a function. One of the first features in this library is the ability to re-use functions as if they are scikit-learn models. Titanic \u00b6 Let's see how this might work. We'll grab a dataset that is packaged along with this library. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) df . head () The df variable represents a dataframe and it has the following contents: survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe. Preparation \u00b6 To prepare our data we will first get it into the common X , y format for scikit-learn. X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense? It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better. FunctionClassifier \u00b6 Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the FunctionClassifier . You can see an example of that below. import numpy as np from hulearn.classification import FunctionClassifier def fare_based ( dataf , threshold = 10 ): \"\"\" The assumption is that folks who paid more are wealthier and are more likely to have recieved access to lifeboats. \"\"\" return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) mod = FunctionClassifier ( fare_based ) This mod is a scikit-learn model, which means that you can .fit(X, y).predict(X) . mod . fit ( X , y ) . predict ( X ) During the .fit(X, y) -step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step. Grid \u00b6 Being able to .fit(X, y).predict(X) is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best threshold value? For that, you might like to use GridSearchCV . from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score , recall_score , accuracy_score , make_scorer # Note the threshold keyword argument in this function. def fare_based ( dataf , threshold = 10 ): return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) # Pay attention here, we set the threshold argument in here. mod = FunctionClassifier ( fare_based , threshold = 10 ) # The GridSearch object can now \"grid-search\" over this argument. # We also add a bunch of metrics to our approach so we can measure. grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'threshold' : np . linspace ( 0 , 100 , 30 )}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) If we make a chart of the grid.cv_results_ then they would look something like; A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a RandomForestClassifier using the 'pclass', 'sex', 'age', 'fare' columns that the precision score would be about the same. Bigger Grids \u00b6 You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that. def last_name ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) # Once again, remember to declare your arguments here too! mod = FunctionClassifier ( last_name , pclass = 10 , sex = 'male' ) # The arguments of the function can now be \"grid-searched\". grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) Conclusion \u00b6 In this guide we've seen the FunctionClassifier . It is one of the many models in this library that will help you construct more \"human\" models. Notebook \u00b6 If you want to download with this code yourself, feel free to download the notebook here .","title":"Function as a Model"},{"location":"guide/function-classifier.html#titanic","text":"Let's see how this might work. We'll grab a dataset that is packaged along with this library. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) df . head () The df variable represents a dataframe and it has the following contents: survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe.","title":"Titanic"},{"location":"guide/function-classifier.html#preparation","text":"To prepare our data we will first get it into the common X , y format for scikit-learn. X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense? It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better.","title":"Preparation"},{"location":"guide/function-classifier.html#functionclassifier","text":"Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the FunctionClassifier . You can see an example of that below. import numpy as np from hulearn.classification import FunctionClassifier def fare_based ( dataf , threshold = 10 ): \"\"\" The assumption is that folks who paid more are wealthier and are more likely to have recieved access to lifeboats. \"\"\" return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) mod = FunctionClassifier ( fare_based ) This mod is a scikit-learn model, which means that you can .fit(X, y).predict(X) . mod . fit ( X , y ) . predict ( X ) During the .fit(X, y) -step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step.","title":"FunctionClassifier"},{"location":"guide/function-classifier.html#grid","text":"Being able to .fit(X, y).predict(X) is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best threshold value? For that, you might like to use GridSearchCV . from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score , recall_score , accuracy_score , make_scorer # Note the threshold keyword argument in this function. def fare_based ( dataf , threshold = 10 ): return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) # Pay attention here, we set the threshold argument in here. mod = FunctionClassifier ( fare_based , threshold = 10 ) # The GridSearch object can now \"grid-search\" over this argument. # We also add a bunch of metrics to our approach so we can measure. grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'threshold' : np . linspace ( 0 , 100 , 30 )}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) If we make a chart of the grid.cv_results_ then they would look something like; A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a RandomForestClassifier using the 'pclass', 'sex', 'age', 'fare' columns that the precision score would be about the same.","title":"Grid"},{"location":"guide/function-classifier.html#bigger-grids","text":"You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that. def last_name ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) # Once again, remember to declare your arguments here too! mod = FunctionClassifier ( last_name , pclass = 10 , sex = 'male' ) # The arguments of the function can now be \"grid-searched\". grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y )","title":"Bigger Grids"},{"location":"guide/function-classifier.html#conclusion","text":"In this guide we've seen the FunctionClassifier . It is one of the many models in this library that will help you construct more \"human\" models.","title":"Conclusion"},{"location":"guide/function-classifier.html#notebook","text":"If you want to download with this code yourself, feel free to download the notebook here .","title":"Notebook"},{"location":"guide/function-preprocessing.html","text":"In python the most popular data analysis tool is pandas while the most popular tool for making models is scikit-learn. We love the data wrangling tools of pandas while we appreciate the benchmarking capability of scikit-learn. The fact that these tools don't fully interact is slightly awkward. The data going into the model has an big effect on the output. So how might we more easily combine the two? Pipe \u00b6 In pandas there's an amazing trick that you can do with the .pipe method. To demonstrate how this works, let's load in the titanic dataset. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] X . head ( 4 ) The X variable represents a dataframe with variables that we're going to use to predict the survival rate (stored in y ). Here's a preview of what X might have. pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code. X [ 'nchar' ] = X [ 'name' ] . str . len () This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function. def process ( dataf ): # Make a copy of the dataframe to prevent it from overwriting the original data. dataf = dataf . copy () # Make the changes dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () # Return the name dataframe return dataf We now have a nice function that makes our changes and we can use it like so; X_new = process ( X ) We can do something more powerful though. Paramaters \u00b6 Let's make some more changes to our process function. def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end. We've changed the way we've defined our function but we're also changing the way that we're going to apply it. # This is equivalent to preprocessing(X) X . pipe ( preprocessing ) The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview. With .pipe() \u00b6 ( df . pipe ( set_col_types ) . pipe ( preprocessing , nchar = True , gender = False ) . pipe ( add_time_info )) Without .pipe() \u00b6 add_time_info ( preprocessing ( set_col_types ( df ), nchar = True , gender = False )) Let's be honest, this looks messy. PipeTransformer \u00b6 It would be great if we could use the preprocessing -function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas .pipe -line in general! For that we've got another feature in our library, the PipeTransformer . from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) # Important, don't forget to declare `n_char` and `gender` here. tfm = PipeTransformer ( preprocessing , n_char = True , gender = True )) The tfm variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function. from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV pipe = Pipeline ([ ( 'prep' , tfm ), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) Once trained we can fetch the grid.cv_results_ to get a glimpse at the results of our pipeline. param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 It seems that we gender of the passenger has more of an effect on their survival than the length of their name. Utility \u00b6 The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code. There's a few small caveats to be aware of. Don't remove data \u00b6 Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this. Don't sort data \u00b6 You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the y variable that you're trying to predict. Don't use lambda \u00b6 There's two ways that you can add a new column to pandas. # Method 1 dataf_new = dataf . copy () # Don't overwrite data! dataf_new [ 'new_column' ] = dataf_new [ 'old_column' ] * 2 # Method 2 dataf_new = dataf . assign ( lambda d : d [ 'old_column' ] * 2 ) In many cases you might argue that method #2 is safer because you do not need to worry about the dataf.copy() that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use lambda functions because it cannot pickle the code. =","title":"Human Preprocessing"},{"location":"guide/function-preprocessing.html#pipe","text":"In pandas there's an amazing trick that you can do with the .pipe method. To demonstrate how this works, let's load in the titanic dataset. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] X . head ( 4 ) The X variable represents a dataframe with variables that we're going to use to predict the survival rate (stored in y ). Here's a preview of what X might have. pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code. X [ 'nchar' ] = X [ 'name' ] . str . len () This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function. def process ( dataf ): # Make a copy of the dataframe to prevent it from overwriting the original data. dataf = dataf . copy () # Make the changes dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () # Return the name dataframe return dataf We now have a nice function that makes our changes and we can use it like so; X_new = process ( X ) We can do something more powerful though.","title":"Pipe"},{"location":"guide/function-preprocessing.html#paramaters","text":"Let's make some more changes to our process function. def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end. We've changed the way we've defined our function but we're also changing the way that we're going to apply it. # This is equivalent to preprocessing(X) X . pipe ( preprocessing ) The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview.","title":"Paramaters"},{"location":"guide/function-preprocessing.html#with-pipe","text":"( df . pipe ( set_col_types ) . pipe ( preprocessing , nchar = True , gender = False ) . pipe ( add_time_info ))","title":"With .pipe()"},{"location":"guide/function-preprocessing.html#without-pipe","text":"add_time_info ( preprocessing ( set_col_types ( df ), nchar = True , gender = False )) Let's be honest, this looks messy.","title":"Without .pipe()"},{"location":"guide/function-preprocessing.html#pipetransformer","text":"It would be great if we could use the preprocessing -function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas .pipe -line in general! For that we've got another feature in our library, the PipeTransformer . from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) # Important, don't forget to declare `n_char` and `gender` here. tfm = PipeTransformer ( preprocessing , n_char = True , gender = True )) The tfm variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function. from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV pipe = Pipeline ([ ( 'prep' , tfm ), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) Once trained we can fetch the grid.cv_results_ to get a glimpse at the results of our pipeline. param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 It seems that we gender of the passenger has more of an effect on their survival than the length of their name.","title":"PipeTransformer"},{"location":"guide/function-preprocessing.html#utility","text":"The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code. There's a few small caveats to be aware of.","title":"Utility"},{"location":"guide/function-preprocessing.html#dont-remove-data","text":"Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this.","title":"Don't remove data"},{"location":"guide/function-preprocessing.html#dont-sort-data","text":"You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the y variable that you're trying to predict.","title":"Don't sort data"},{"location":"guide/function-preprocessing.html#dont-use-lambda","text":"There's two ways that you can add a new column to pandas. # Method 1 dataf_new = dataf . copy () # Don't overwrite data! dataf_new [ 'new_column' ] = dataf_new [ 'old_column' ] * 2 # Method 2 dataf_new = dataf . assign ( lambda d : d [ 'old_column' ] * 2 ) In many cases you might argue that method #2 is safer because you do not need to worry about the dataf.copy() that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use lambda functions because it cannot pickle the code. =","title":"Don't use lambda"}]}